<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>计算机网络 CH1</title>
    <link href="/2025/09/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-CH1/"/>
    <url>/2025/09/17/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-CH1/</url>
    
    <content type="html"><![CDATA[<h1 id="计算机网络-ch1概述">计算机网络 CH1：概述</h1><h2 id="因特网概述">1.2 因特网概述</h2><h3 id="网络互联网与因特网">1.2.1 网络，互联网与因特网</h3><p><img src="/计算机网络pictures/image-79.png" alt="alt text" /> 1. <strong>网络</strong>：关注概念：连接的设备称为<strong>节点（node）</strong>，连接设备的称为<strong>链路（link）</strong>；节点和链路组成的结构就是网络。 2. <strong>互联网</strong>：网络与网络之间，一般通过<strong>路由器</strong>来连接；将一系列的网络连接起来，就形成<strong>互联网</strong>，如下图所示；互联网是“网络的网络” <img src="/计算机网络pictures/image-80.png" alt="alt text" /> 3. <strong>因特网（Internet）</strong>：因特网是世界上最大的互联网，连接到因特网的设备称为<strong>主机（host）</strong>，如图所示<img src="/计算机网络pictures/image-81.png" alt="alt text" /> 4. 互联网与因特网的对比：<img src="/计算机网络pictures/image-82.png" alt="alt text" /> 5. <strong>协议</strong>：先理解为是一种网络设备之间通讯所遵守的一系列的规则与约定（比如说，规定了数据传输的格式和结构，传输的规则，建立和关闭连接的形式，etc.） ### 1.2.2 因特网简介 1. <strong>因特网服务提供者（Internet Service Provider，ISP）</strong> 普通用户与ISP（付费）签约，以使用互联网；ISP是为用户提供接入互联网服务的组织；由于互联网的结构复杂，因此需要ISP帮助个人用户连接。 目前为止，因特网已经发展为<strong>基于ISP的多层次结构的互联网络</strong> <img src="/计算机网络pictures/image-83.png" alt="alt text" /> 2. <strong>因特网的标准化工作</strong>：因特网的标准化工作是面向大众的，对于互联网标准提出的建议标注称为<strong>RFC</strong>，后续需要进一步评估才可能有效、被采纳 3. <strong>因特网的组成</strong>：如图所示：<img src="/计算机网络pictures/image-84.png" alt="alt text" /></p><h2 id="电路交换分组交换和报文交换">1.3 电路交换，分组交换和报文交换</h2><h3 id="电路交换">1.3.1 电路交换</h3><p>电路交换是指在通信双方进行通信之前，首先在通信网络中为其建立一条专用的通信路径（电路），该路径在通信过程中一直保持，直到通信结束后才释放。这种方式常见于传统的电话网络中。用图表述如下：<img src="/计算机网络pictures/image-85.png" alt="alt text" /> <div class="note note-danger">            <p>电路交换是显然不适用于计算机交互的；比如说，使用微信和别人发消息，很可能是很长时间之发一两条消息，但如果你电路交换一直占着线路，但可能大部分时候网络实际上是空闲的，这很容易导致大量的资源的浪费；这是由于计算机信息的交换一般具有<strong>突发性</strong></p>          </div></p><h3 id="分组交换">1.3.2 分组交换</h3><p>分组交换的基本结构如下图：<img src="/计算机网络pictures/image-86.png" alt="alt text" /> 假如说我H1与H3要进行传输文件，这个时候我们先将信息（的0/1编码）分成很多等长的message，分组完之后如下所示：<img src="/计算机网络pictures/image-87.png" alt="alt text" />其中，<strong>“首部”</strong> 记录信息的寄件人和收件人；信息传输完成后，在收件人处删除掉首部，重新组装信息。 收件Host和寄件Host之间通过一系列路由器（R）进行存储转发，路由器会先存储信息，然后转发给下一个路由器。分组交换的优缺点如下所示：<img src="/计算机网络pictures/image-88.png" alt="alt text" /></p><h3 id="报文交换">1.3.3 报文交换</h3><p>报文交换是分组交换的前身，区别就是报文交换是整个地发送，而不是分组发送；路由器会将文件整个接受后才会存储转发。 因此，很显然报文交换带来的转发时延和需求的存储空间都会大很多。</p><p>总结一下这部分的内容如下：<img src="/计算机网络pictures/image-89.png" alt="alt text" /></p><h2 id="计算机网络的定义和分类">1.4 计算机网络的定义和分类</h2><h3 id="计算机网络的定义">1.4.1 计算机网络的定义</h3><p>在不同的时期，计算机网络有不同的定义；现阶段来说，较好的定义是<strong>计算机网络主要是由一些通用的、可编程的硬件互连而成的</strong>，而这些硬件都可以用来传输多种类型的数据，支持广泛的应用。 ## 1.4.2 计算机网络的分类 （1）按照交换方式：电路交换、报文交换、分组交换 （2）按照使用者：公用网和专用网（e.g. 军队、铁路、银行,etc.） （3）按照传输介质：有线网和无线网 （4）按照覆盖范围：如下<img src="/计算机网络pictures/image-91.png" alt="alt text" /> （5）按照拓扑结构：有总线型、网状型、环形、星形，etc.</p><h2 id="计算机网络的性能指标">1.5 计算机网络的性能指标</h2><ol type="1"><li><strong>速率</strong>：速率就是指每秒可以传输多少个比特，也称为数据率（data rate）或者是比特率（bit rate）。 <div class="note note-primary">            <p>小写的b表示bit，大写的B表示byte。</p>          </div></li></ol><div class="note note-warning">            <p>有一个很恶心的东西，K M B T四个量在表示数据量和速率的时候代表的含义不同，如图所示：<img src="/计算机网络pictures/image-92.png" alt="alt text" /> 也就是说，数据量是采用二进制的2的次方，速率是一般的10的次方。参见下面的例题：<img src="/计算机网络pictures/image-93.png" alt="alt text" /></p>          </div><ol start="2" type="1"><li><strong>带宽(bandwidth)</strong>: 在模拟信号和计算机网络中有不同的定义，如图所示：<img src="/计算机网络pictures/image-94.png" alt="alt text" />关键词：最大 在数据传输的过程中，有如下的木桶效应：<img src="/计算机网络pictures/image-95.png" alt="alt text" /></li><li><strong>吞吐量(throughput)</strong>: 吞吐量是指<strong>在单位时间内通过某个网络或接口的实际数据量</strong>，如图所示为通过线路带宽的吞吐量<img src="/计算机网络pictures/image-96.png" alt="alt text" /></li><li><strong>时延(latency)</strong>:时延是指数据从网络的一端传送到另一端所耗费的时间,在传输过程中各个部分的时延和计算方式如下图所示：<img src="/计算机网络pictures/image-97.png" alt="alt text" /> 电脑、路由器发射数据都会产生发送时延，在线路上会产生传播时延，路由器处理的时候会产生排队时延和处理时延 <div class="note note-warning">            <p>要记住在光纤中的信号传播速度</p>          </div> 用一张图来示意这个部分：<img src="/计算机网络pictures/image-98.png" alt="alt text" />一般来说，结构是这样的（忽略了中间的排队时延和处理时延） <div class="note note-example">            <p>一个例题：<img src="/计算机网络pictures/image-99.png" alt="alt text" /></p>          </div></li></ol>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>OS CH1:introduction</title>
    <link href="/2025/09/16/OS-CH1-introduction/"/>
    <url>/2025/09/16/OS-CH1-introduction/</url>
    
    <content type="html"><![CDATA[<h1 id="os-ch1introduction">OS CH1:introduction</h1><h2 id="linux概述">1.0 LINUX概述</h2><p><img src="/osch1pictures/image-75.png" alt="alt text" /> <img src="image-105.png" alt="alt text" /> 如图为Linux发展的“五大支柱”、 ### 1.0.1 LINUX的系统结构 Linux系统中没有像windows的不同的C D E盘之类的结构，而是统一挂在同一个序列下面 需要掌握Linux相关的命令！</p><h2 id="操作系统是做什么的">1.1 操作系统是做什么的？</h2><p>操作系统是一个<strong>介于计算机hardware和user之间的“中介”</strong>。 <div class="note note-primary">            <p>这里的user并非一定是“人”，也可以是“软件”，连接到主机的笔记本，etc.</p>          </div> 操作系统有这些作用：<img src="/osch1pictures/image-73.png" alt="alt text" /> 从架构的角度来看：<img src="/osch1pictures/image-74.png" alt="alt text" /> 从计算机系统组成的角度看（软硬件）：操作系统是<strong>系统软件</strong> 从用户的角度看：操作系统是硬件和用户的接口，其提供了<strong>命令级接口</strong>（e.g. 鼠标，触摸屏，键盘，etc.）与<strong>程序级接口</strong>（操作系统给一系列程序提供调用的接口，也就是程序通过操作系统才可以接触、调用底层硬件的东西） 从系统的角度来看：操作系统是计算机系统资源的管理者（管理各种乱七八糟的硬件）</p><h2 id="计算机系统的组成">1.2 计算机系统的组成</h2>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>os</title>
    <link href="/2025/09/16/os/"/>
    <url>/2025/09/16/os/</url>
    
    <content type="html"><![CDATA[<h1 id="os-ch0">OS ch0</h1><p>1.inux内核部分（大概率）不考 2.课程考核标准 <img src="image-72.png" alt="alt text" /> 3.代码注释用中文！</p>]]></content>
    
    
    <categories>
      
      <category>操作系统</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RL5：策略梯度进阶；策略优化算法</title>
    <link href="/2025/09/15/RL5/"/>
    <url>/2025/09/15/RL5/</url>
    
    <content type="html"><![CDATA[<h1 id="rl5策略优化算法">RL5：策略优化算法</h1><div class="note note-primary">            <p>对前面一章内容的补充，以及引入： <img src="/RL5pictures/image-106.png" alt="alt text" /> 如图为策略网络，如果输入的为向量那么省略前面两层；否则，可以利用卷积网络先将张量转化为特征向量，然后接入全连接网络。</p>          </div><h2 id="trpo">5.1 TRPO</h2><p><strong>TRPO</strong> 的全称为<strong>Trust Region Policy Optimization</strong>，中文翻译为<strong>信任区间策略优化</strong>。TRPO算法具有两个很明显的优势：其表现更稳定、对学习率不敏感，并且其需要的训练经验更少。 ### 5.1.1 置信域方法 理解TRPO的基础就是这个所谓的置信域方法。<strong>置信域</strong>的概念最先在优化问题中提出，比如说想要利用<span class="math inline"><em>θ</em></span>来优化<span class="math inline"><em>J</em>(<em>θ</em>)</span>，一般来说优化过程是不断更新当前的<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>。对于置信域方法的具体描述如下：<img src="/RL5pictures/image-111.png" alt="alt text" />也就是说，在这个参数<span class="math inline"><em>θ</em></span>的邻域内，我们构造了这个一个函数<span class="math inline"><em>L</em></span>，使得在这个区间上可以用L来替代J；置信域需要伴随着构造的函数一同存在才有意义。 置信域方法的意义就在于，大多数时候我们的目标函数<span class="math inline"><em>J</em>(<em>θ</em>)</span>形式上会很复杂，但是如果我们能找到一个可以值得我们“信任”的形式上更简单的函数<span class="math inline"><em>L</em>(<em>θ</em>)</span>，那么我们就可以直接优化这个函数来达到目的即可。 所谓的置信域方法的核心在于如下的两个步骤： 1. 给定<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>,确定使用的邻域，找到用于近似的函数<span class="math inline"><em>L</em></span> 2. 在这个区间内，优化参数<span class="math inline"><em>θ</em> = <em>a</em><em>r</em><em>g</em><em>m</em><em>a</em><em>x</em>(<em>L</em>(<em>θ</em>))</span>，这个<span class="math inline"><em>θ</em></span>就是这一步优化得到的<span class="math inline"><em>θ</em><sub><em>n</em><em>e</em><em>w</em></sub></span>。随后重复这两个步骤，继续优化的过程。 ### 5.1.2 数学推导 TRPO的核心在于如下的几个等式：<img src="/RL5pictures/image-112.png" alt="alt text" /><img src="/RL5pictures/image-113.png" alt="alt text" /><img src="/RL5pictures/image-114.png" alt="alt text" /></p><p>第一个式子就是把一个状态的价值拆分成各个动作的期望回报的加权和； 第二个式子在分子和分母上同时拆出了一项<span class="math inline"><em>π</em>(<em>a</em>|<em>s</em>; <em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub>)</span>，然后把总的期望项改成了关于现在参数<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>的期望项； 第三个式子就是由于<span class="math inline"><em>J</em>(<em>θ</em>)</span>是关于所有状态的期望和，根据第二个式子简单变形得到。 但是，在具体的操作过程中，肯定要做一些近似工作，这里我们外面两项的期望分别是对状态和动作而言的，因此我们尝试使用蒙特卡洛方法进行模拟时，采取这样的方法：我们使用当前的策略<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>完整地玩一局游戏，会得到一系列的动作-状态-奖励序列。由于每一个状态都是从环境中观测到的，而动作也是按照当前的策略进行选择的，因此<img src="/RL5pictures/image-115.png" alt="alt text" />就可以看成是9.1式子的无偏估计。因此，对于以上的这个结果进行n项的求平均，就也是一个无偏估计。因此综上，得到如下的内容：<img src="/RL5pictures/image-116.png" alt="alt text" /> 但是，由于对于环境的未知性，这里的<span class="math inline"><em>Q</em><sub><em>π</em></sub></span>还是不方便求出，因此还需要对于Q进行估计。在一个置信域中，可以如下的方法进行两次估计：<img src="/RL5pictures/image-117.png" alt="alt text" />因此，最后的最后，在我们的置信域中，有如下的估计成立<img src="/RL5pictures/image-118.png" alt="alt text" />，这就是我们接下来需要优化的函数。 <div class="note note-warning">            <p>这里对于Q的第二步近似，是把新参数的新策略近似为了旧的策略，因此这个强调了置信域的重要，不然这个近似的差距会很大。</p>          </div> ### 5.1.3 最大化 最大化的过程很显然如下：<img src="image-120.png" alt="alt text" /> #### 5.1.3.1 置信域的选择 对于置信域的选择，主要有两种主流的方法： 1. 选择一个以当前的<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>为球心的高维球，也就是说：选定一个半径,问题转化为：<img src="image-121.png" alt="alt text" /> 这个方法的好处是置信域十分简单明了，但是实操起来效果并不是很好 2. 第二种方法是借助KL散度的方法。 <div class="note note-primary">            <p>KL散度具有如下的定义：<img src="image-122.png" alt="alt text" /></p>          </div> 从这个定义就可以看出，这十分直观：我们需要限制选定的基点<span class="math inline"><em>θ</em><sub><em>n</em><em>o</em><em>w</em></sub></span>与我范围内的<span class="math inline"><em>θ</em></span>形成的策略的散度值相差不能太大，否则的话我的策略就会有很大的偏差，L的估计值便显得不合理。数学地写出来如下：<img src="image-123.png" alt="alt text" /></p><h4 id="如何计算">5.1.3.2 如何计算？</h4><p>信奉这样一句话：<img src="image-124.png" alt="alt text" />也就是说，会有具体的计算方法，了解到这个程度就够了（）</p><h3 id="训练流程总结">5.1.4 训练流程总结</h3><p><img src="image-125.png" alt="alt text" /> 这里也说到，TRPO算法虽然理论很先进，具有很多优点，但是由于上面带约束优化问题本身的复杂度，导致其真正实现起来还是有一定的困难。</p><h2 id="ppo">5.2 PPO</h2><p><strong>PPO</strong>的全称为<strong>Proximal Policy Optimization</strong>，中文意为<strong>近端策略优化</strong>。 从我自己先前的感觉来看，PPO应当是目前应用最多、效果也最好的一个算法了。</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习，笔记，PPO，TRPO</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算理论 CH1</title>
    <link href="/2025/09/15/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA-CH1/"/>
    <url>/2025/09/15/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA-CH1/</url>
    
    <content type="html"><![CDATA[<h1 id="ch1-sets-relations-and-languages">CH1: Sets, Relations and Languages</h1><h2 id="sets">1.1 Sets</h2><div class="note note-primary">            <p>learn some English： 1. proper set 真子集 2. partition 分割（不相交的分解） 3. one-to-one function 单射 4. onto function 满射 5. one-to-one correspondence 一一映射</p>          </div><p>1.recall：symmetric &amp; anti-symmetric:反自反意思是，如果a-&gt;b(a!=b) 那么就不能有b-&gt;a</p><p>2.<strong>partial order 偏序</strong>：自反，传递，反对称（有大小关系）</p><p>3.recall： 在一个偏序关系中： <strong>least element</strong>：对所有的b，都有a&lt;=b <strong>minimal element</strong>：如果a&lt;=b，那么a=b 因此，least更严格，因为要对每一个元素其都要在它的左边。类似地，定义了greatest和maximal</p><p>4.<strong>total order 全序</strong> 指的是在一个偏序关系中，对于任意一个pair<span class="math inline">(<em>a</em>, <em>b</em>)</span>，要么有<span class="math inline"><em>a</em> &lt;  = <em>b</em></span>，要么有<span class="math inline"><em>b</em> &lt;  = <em>a</em></span>；也就是说，所有的都得连起来</p><p>5.<strong>equinumerous（等势）</strong> 如果两个集合等势，那么存在他们俩之间的一个双射。等势关系“~”显然是一个等价关系</p><p>6.<strong>Cardinality（基数）</strong>：集合的基数，也就是集合中元素的数量。等势代表 <span class="math inline">|<em>A</em>| = |<em>B</em>|</span>。 无穷集合也有基数，使用定义好的符号表示 进一步地，recall finite/infinite set, countable/uncountable finite</p>]]></content>
    
    
    
    <tags>
      
      <tag>计算理论，导论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>计算理论 CH0</title>
    <link href="/2025/09/15/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA-9-15/"/>
    <url>/2025/09/15/%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA-9-15/</url>
    
    <content type="html"><![CDATA[<h2 id="计算理论-ch0">计算理论 CH0</h2><h3 id="intro">1. intro</h3><p>计算理论回答计算机的<strong>基础问题</strong>： 1. 什么是算法？ 2. 什么是可计算的（computable）？ 3. 如何衡量计算的复杂度？ 这门课回答这三个问题，核心的基石是<strong>图灵机</strong>：一种抽线的数学模型 <div class="note note-primary">            <p>如果一个“东西”可以用图灵机的某种规则进行模拟，即为“算法” 如果一个东西可以用图灵机模拟求解，即为“computable” 如果一个东西满足图灵机的某些性质，那么即可以达到、计算其复杂度</p>          </div> 后五次课关于图灵机的最重要！！</p>]]></content>
    
    
    
    <tags>
      
      <tag>计算理论，导论</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>读论文: LSTM: A Search Space Odyssey</title>
    <link href="/2025/09/07/%E8%AE%BA%E6%96%87-LSTM-A-Search-Space-Odyssey/"/>
    <url>/2025/09/07/%E8%AE%BA%E6%96%87-LSTM-A-Search-Space-Odyssey/</url>
    
    <content type="html"><![CDATA[<h1 id="lstm-a-search-space-odyssey">LSTM: A Search Space Odyssey</h1><p><a href="/LSTM/LSTM%20A%20Search%20Space%20Odyssey.pdf">原论文pdf</a></p><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">spinning up的相关学习地址</a></p><p>这是一篇关于LSTM的论文；Odyssey，引申意自《奥德赛》（吗？），意为“艰难的跋涉，漫长的旅程”</p><h2 id="abstract">（0）abstract</h2><div class="note note-primary">            <p><strong>Learn some English</strong> 1. state-of-the-art: 目前最好的，最先进的</p><ol start="2" type="1"><li><p>inception: 开端，创始</p></li><li><p>utility: 帮助，实用性</p></li><li><p>variants: 变体，变种</p></li></ol>          </div><p>从摘要中总结看出，所谓的LSTM（长短期记忆）是一种RNN结构的变体，主要解决了RNN中常出现的<strong>梯度爆炸</strong>的问题；这篇文章主要使用了<strong>随机搜索</strong>方法探索了最优的超参数设置，发现了现有的LSTM变体中，没有模型可以很显著提升当前的标准LSTM架构。</p><h2 id="introduction">（1）Introduction</h2><div class="note note-primary">            <p><strong>Learn some English</strong> 1. hurdle: 障碍，困难</p><ol start="2" type="1"><li><p>plague: 困扰，折磨(做v.时的意思)</p></li><li><p>acoustic: 声学的</p></li><li><p>incorporate: 集成，包含</p></li></ol>          </div><p>其实主要内容于摘要很接近，稍微详细解释了一点 ## (1.5) 最基本的LSTM结构 在阅读下一个部分之前，首先来看一下LSTM模型的最基本的结构：<img src="/LSTM/image-55.png" alt="alt text" />也就是说，与普通的RNN相比，其内部连接的结构会复杂得多。具体来说，分为以下几块详细进行讲解： <img src="/LSTM/image-57.png" alt="alt text" /> （0）LSTM的核心部分，比传统的RNN多的一个核心的地方就是引入了一个类似于“日记本”的结构<strong>Cell State</strong>，这个部分贯穿整个网络始终，并且只与网络作简单的线性交互。可以这么理解：RNN只能有短期的“记忆”，但是引入的这个“日记本”却能让我本来“健忘”的神经网络可以有一个长期的参照。 对于这个结构的修改，通过gate来进行，就是图上红色的圈圈部分。对于输入的内容，通过门和激活函数来控制Cell state中内容的变化。 <img src="/LSTM/image-56.png" alt="alt text" /> （1）LSTM的第一个部分被称为所谓的<strong>forget gate（忘记门）</strong> ，用于决定什么样的信息我们需要保留/删除：其接收前一个时间的输出<span class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>和当前时间的输入<span class="math inline"><em>x</em><sub><em>t</em></sub></span>作为输入，并且使用sigmoid作为激活函数，输出一个0~1之间的值。 具体的计算来看，通过一个矩阵<span class="math inline"><em>W</em><sub><em>f</em></sub></span>和向量<span class="math inline"><em>b</em><sub><em>f</em></sub></span>，将<span class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>和<span class="math inline"><em>x</em><sub><em>t</em></sub></span>连接之后的结果通过计算之后得到一个向量<span class="math inline"><em>f</em><sub><em>t</em></sub></span>，这个部分的意思就是，通过当前时间的输入和上一个时刻的内容，来决定之前我的“日记”中哪些内容是需要被删除的。 <img src="/LSTM/image-58.png" alt="alt text" /> （2）有了前面的forget，下面一个就是<strong>storage gate（输入门）</strong> 了。如图所示，对于<span class="math inline"><em>h</em><sub><em>t</em> − 1</sub></span>和<span class="math inline"><em>x</em><sub><em>t</em></sub></span>，通过sigmoid函数以及tanh函数进行计算之后，来决定哪些有用的内容是需要被我添加到“日记”中。 <div class="note note-success">            <p>为了更方便理解，以使用LSTM理解文本为例，在我们的网络中有两种不同的向量： 1. 单词编码得到的向量，这是实现人为定义好的（比如说，使用word2vec等等技术），输入向量是人为确定的嵌入内容 2. 文本向量，也就是<span class="math inline"><em>C</em><sub><em>t</em></sub></span>，这个向量的维度是设定的超参数（通常与之前的维度相同，或者是512？）各个维度代表的意义是在模型训练中自己学习得到的 在LSTM计算的过程中，会输入嵌入过的词向量，然后根据计算的方法来决定在我的cell中有原有的向量各个维度需要被增加或者删除多少，在后续的步骤中进行加权计算后更新。</p>          </div> <img src="/LSTM/image-59.png" alt="alt text" /> （3）第三个步骤就是根据上面计算的内容，相应更新<span class="math inline"><em>C</em><sub><em>t</em> − 1</sub></span>到下一个state<span class="math inline"><em>C</em><sub><em>t</em></sub></span>了。如图所示的计算，前面计算的向量<span class="math inline"><em>f</em><sub><em>t</em></sub></span>就是上一个状态遗忘之后的权重，因此乘以<span class="math inline"><em>C</em><sub><em>t</em> − 1</sub></span>，代表前一个状态被保留下来的部分;storage gate中计算的input值<span class="math inline"><em>i</em><sub><em>t</em></sub></span>与Candidate value<span class="math inline">$\tilde{C_t}$</span>相乘，作为当前时刻新增的输入值增加到日志中。（本质上看，相应更新的内容都是维度的权重） <img src="/LSTM/image-60.png" alt="alt text" /> （4）最后一个部分就是所谓的<strong>output gate（输出门）</strong> 了，这个部分输出当前对应的神经网路输出的<span class="math inline"><em>h</em><sub><em>t</em></sub></span>；首先使用sigmoid进行和上面类似的矩阵类型的计算，然后乘以tanh函数（把这个值调整到<span class="math inline">[ − 1, 1]</span>区间之间），之后输出得到下一个状态的h值。 到这里为止，一个基本形态的LSTM网络就完成了。 <div class="note note-success">            <p>在先前的LSTM模型中，主要使用到的激活函数均为sigmoid与tanh，这是根据他们不同的特性决定的：sigmoid输出值为0~1之间，这样方便表示保留的权重信息（也就是说，百分之多少被保留，etc.）；而tanh具有在对称区间输出的特点，因此，再比如说candidate value的地方使用tanh进行激活，可以相应地记录保留的什么值是“正向”或是“负向”的。</p>          </div></p><h2 id="vanilla-lstm">（2）VANILLA LSTM</h2><p>vanilla不仅仅是熟悉的“香草”意思，作形容词的时候还有“普通的，平淡无奇”之意！ <img src="/LSTM/image-62.png" alt="alt text" /> 从这篇文章中对于vanilla LSTM的定义可以看出，这些权重矩阵</p>]]></content>
    
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>LSTM</tag>
      
      <tag>RNN</tag>
      
      <tag>论文</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>ptcg</title>
    <link href="/2025/08/31/ptcg/"/>
    <url>/2025/08/31/ptcg/</url>
    
    <content type="html"><![CDATA[<h2 id="关于dataptcg下各文件的用途和关系">1. 关于data/ptcg下各文件的用途和关系</h2><h3 id="agent-workflow-memory不是这个项目的">1.1 agent-workflow-memory（不是这个项目的？）</h3><p>AWM(代理工作流) 是一种预先定义或者存储的agent的“工作蓝图”，其可以看成是agent的“记忆”，用于指导LLM完成各种复杂的任务。<img src="/ptcgpictures/image-40.png" alt="alt text" /> 该父文件夹下的 mind2web以及webarena应该都是附带的基准测试文件</p><h3 id="claudeplayspokemonstarter">1.2 ClaudePlaysPokemonStarter</h3><p>这是一个用于将Claude AI用于play pokemon的红方的智能体系统</p><h3 id="open-spiel">1.3 open spiel</h3><p>“OpenSpiel 是 DeepMind 开发的一个专注于游戏强化学习研究的开源框架。它提供了一个丰富的游戏环境集合和算法实现，支持多种类型的游戏和研究场景。” 可以理解为是游戏强化学习的一个平台</p><h3 id="open-ptcg">1.4 open-ptcg</h3><p>应该是主要的工程文件，里面除了包含了之前的整个ptcg的信息之外，还有一些额外的强化学习相关的内容。其中： #### 1.4.1 eval文件夹 eval文件夹中导入了对于训练好的模型进行评估的手段，其中各个文件的作用分别为： ##### (1) eval_sb3_ppo.py 这个文件是用来在训练的过程中定性观察训练出的agent在过程中采取的策略的文件，其对于单个环境进行可视化的演示，方便逐步调试以及观察游戏进程 ##### (2) eval_sb3.py 这个文件用于加载已经训练好的模型，并且评估模型在指定数量的episode中的表现，并且输出得到的奖励结果 ##### (3) eval_all.py 这是一个全方位的评估文件，可以用于多种智能体之间的对比和对战，其可以：<img src="/ptcgpictures/image-41.png" alt="alt text" />同时，创建出一个N*N的不同agent的对战胜利矩阵，以及跑完之后完整的wandb的实验记录和结果分析 ##### (4) evaluate.py 这个文件实现了一个交互式的游戏演示系统，其可以让你实现与AI对战或者实时观察两个智能体之间的战斗过程，是一个与前端相结合的交互式系统</p><h4 id="logsmodels-文件夹">1.4.2 logs&amp;models 文件夹</h4><p>这两个文件夹应该是记录之前（以及之后）训练结果以及对战的各条log和记录</p><h4 id="关于核心的代码架构">1.4.3 关于核心的代码架构</h4><h5 id="关于action-space的定义">（1）关于action space的定义</h5><p>在目录open-ptcg/ptcg/core下有一个spaces.py文件，里面对于action space有如下的定义： <figure class="highlight livescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs livescript">def _action_space<span class="hljs-function"><span class="hljs-params">()</span> -&gt;</span> spaces.MultiDiscrete:<br>    <span class="hljs-comment"># 0 ~ 14: one-hot encoding for action type [0-1]</span><br>    <span class="hljs-comment"># 15: source card id [0-32]</span><br>    <span class="hljs-comment"># 16: target card id [0-32]</span><br>    nvec = [<span class="hljs-number">2</span>] * <span class="hljs-number">15</span> + [<span class="hljs-number">33</span>] * <span class="hljs-number">2</span> + [<span class="hljs-number">2</span>] * <span class="hljs-number">20</span><br>    <span class="hljs-keyword">return</span> spaces.MultiDiscrete(nvec)<br></code></pre></td></tr></table></figure> 在这一小段代码中，有如下几个解释的地方： （1） <strong>space.MultiDiscrete</strong>：这是Gym中的一个类型，表示一个多维的离散动作空间。其基本的定义类型如下： space.MultiDiscrete([<span class="math inline"><em>n</em><sub>1</sub></span>,<span class="math inline"><em>n</em><sub>2</sub></span>,…,<span class="math inline"><em>n</em><sub><em>k</em></sub></span>])，表示定义的动作空间一共有k个维度，其中第<span class="math inline"><em>i</em></span>个维度的取值范围分别是 0 到 <span class="math inline"><em>n</em><sub><em>i</em></sub> − 1</span>。 （2）<strong>nvec</strong> : 其实就是number of vectors的缩写，与上面的离散动作空间结合使用，具体来说，在上面的代码中表示：有15个取<span class="math inline">[0, 1]</span>的空间，2个取<span class="math inline">[0, 32]</span>的空间，以及后面跟着的20个取<span class="math inline">[0, 1]</span>的空间。 也就是说，把整个的动作空间抽象成了这样的一个37维的向量。对于其取值的具体含义，在open-ptcg/ptcg/core/action.py中有详细的说明： （1）（补充知识） abc(abstract base class) 抽象基类，是python中用于创建抽象类的一种机制。这里定义了class action(ABC) （2）在action类中，进行了encode编码，最后得到了一个37维度的numpy数组，具体来说进行了如下的划分： <strong>维度0：玩家id</strong> 进行了如下的处理： <figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs stylus">action = np<span class="hljs-selector-class">.zeros</span>(<span class="hljs-number">17</span>, dtype=np.int8)<br><br>action<span class="hljs-selector-attr">[0]</span> = self<span class="hljs-selector-class">.playerId</span><span class="hljs-selector-class">.value</span> - <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure> 也就是说，player1即为0，player2即为1 <strong>维度1-14：动作类型的one-hot编码</strong> 这些维度根据同一个玩家在ptcg中可能执行的各种不同的动作进行编码，具体来说对应关系如下：<img src="/ptcgpictures/image-44.png" alt="alt text" /> 对于每一种动作，对应关系是这样的： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">elif</span> <span class="hljs-built_in">isinstance</span>(<span class="hljs-variable language_">self</span>, UseStadiumAction):<br>    action[<span class="hljs-number">4</span>] = <span class="hljs-number">1</span><br></code></pre></td></tr></table></figure> 即，每一个动作对应相应的位为1，其余为全0 <strong>维度15、16：源卡的id与目标卡的id</strong> 卡牌的索引在card_list列表中有相应的关系，这两个维度分别都有0-32的取值范围；维度16当且仅当动作有目标的时候才会设置；这两个位置的有效值均从1开始（？ <strong>维度17-36：选卡信息</strong> 这些维度当且仅当动作为ChooseCardsAction的时候才有用，表示选卡最多从20张中进行选择，选择哪些卡片对应的位置就会被置为1，否则保持为0表示不选</p><p>以上为action定义的第一部分编码部分</p><p>接下来的第二部分to_nl展示了如何把这些动作部分转化为易于直接阅读的log，与核心rl关系不大；后续的也多为基础架构的信息 简而言之，这个部分告诉了我们，玩家的每一回合做出的每一张卡牌的可能的动作都可以被转化为一个37维度的“向量”，方便后续的操作</p><h5 id="关于不同state的定义">(2) 关于不同state的定义</h5><p>在目录open-ptcg/ptcg/core下有一个state.py文件，里面对于state的实现有如下定义： (0)首先是一段命名逻辑的转换： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_c2s</span>(<span class="hljs-params">name</span>):<br>    s1 = re.sub(<span class="hljs-string">&quot;(.)([A-Z][a-z]+)&quot;</span>, <span class="hljs-string">r&quot;\1_\2&quot;</span>, name)<br>    s2 = re.sub(<span class="hljs-string">&quot;([a-z0-9])([A-Z])&quot;</span>, <span class="hljs-string">r&quot;\1_\2&quot;</span>, s1)<br>    <span class="hljs-keyword">return</span> s2.lower()<br></code></pre></td></tr></table></figure> 这段代码作用是，将原来的名字全部转化为全小写和下划线连接的形式，比如：PlayerId -&gt; player_id，等等 接下来的代码分为如下的几个部分： （1）<strong>init方法</strong>：初始化关于定义的state的各个信息 （2）<strong>get_area方法</strong>：其接受一个area参数（其中area[0] area[1]等等内容应该为玩家的ID 卡牌的位置等等信息），返回的是指定的这个area区域的所有卡牌，返回的形式通常如下：<img src="/ptcgpictures/image-46.png" alt="alt text" />会返回一个cardlist类型的对象，包含这个区域所有卡牌的信息 （3）<strong>def_todict方法</strong>：这个部分的作用是把玩家和当前场上的信息转化为一个序列化的字典结构（类似于json的结构），转化为类似如下的格式（同时，借助于上面的c2s实现标准化的文本转化）：<img src="/ptcgpictures/image-47.png" alt="alt text" />这个的目的应该是方便给前端传递json文件（？ (4)<strong>get_encoded_obs函数</strong>：这应该是与强化学习有关的比较核心的一个部分，用于将游戏状态编码成可以输入给强化学习环境的编码值，具体的实现分为以下的几个部分： 1. encode_pokemon(card)：这个部分输入一张宝可梦卡牌，接着将其编码为一个长度为15的数组，当输入的卡牌信息的确为宝可梦卡牌时，会在不同的地方存储相应的信息，15个格子分别代表宝可梦的血量（除以10）、是否有攻击、攻击的伤害、宝可梦需要的能量种类和所需要的点数（不同的格子代表不同属性的能量，格子存储的值代表这种属性能量需要的点数）。在这个部分的后面，会接上一个代表这张卡牌编号的one-hot编码；也就是说，最后的宝可梦卡牌由两个部分组成，前一半是宝可梦的属性信息，后一半是宝可梦的编号信息 2. encode_card(card)：对于其他的非宝可梦卡牌，直接使用one-hot编码表示。</p><p>以上的两个函数都在父函数encode_player(player)下面，这个函数下面还有以下的一些组成： 1.meta_info 编码：里面存储了一系列表示场上卡牌信息的内容，包括了当前是哪个玩家的回合、各个区域的卡牌数量以及各个玩家各个区域的卡牌数量等信息（纯记录数量的信息） 2.encode_card_list 和 encode_pokemon_list：用于对一个卡组进行编码，定义一个max_size，遍历card_list的所有卡片，对其中的每一张卡调用上面的方法生成编码；若卡牌数量小于max_size，那么空的地方用empty_card填充。（代码中定义了方法，即生成全为0的编码）</p><p>因此，这个state.py最终返回的信息是对于两个玩家，分别返回对应的字典，其中包含了meta_info以及一系列的编码后的卡牌向量信息。</p><div class="note note-primary">            <p>总结一下，前两个部分分别展现了在ptcg卡牌的强化学习任务中的动作和状态是如何通过将其编码进行描述的，简单来说就是进行多采用one-hot的编码进行规范化的传递</p>          </div><h5 id="关于reward的定义">(3)关于reward的定义</h5><p>关于reward的定义在目录open-ptcg/ptcg/core下的reward.py中,具体可以自己修改&amp;设置（比如说，对于充能操作，其奖励应该是在充满能量之后会设置为0，也就是说应该只有当前没有充满的时候，才会有相应的奖励（？</p><h2 id="下面何去何从">2. 下面何去何从？</h2><p>开始的时候，学长就给出了如下的指导：<img src="/ptcgpictures/324ab7cb4b82ea8ae37a2abb6019bdea.png" alt="alt text" /> 以这个为基础，我觉得目前面临的主要是以下三个挑战： 1. 卡牌问题和普通的RL问题很大的区别便是非马尔可夫性，也就是说，卡牌不同于我训练legged robot等等，没走一步都与之前的行为无关，而我的agent在玩卡牌的时候你先前步骤的决策和行为会影响后面的状态（和行为）（比如说，一个最简单的例子，我当前回合充了一个能量，接下来是否可以attack就取决于我之前的某个状态下的充能行为） 因此，对于这个“非马尔可夫性”，我是直接忽略这个问题，还是采取一定的方法解决它？ 2. feature engineering 目前大量利用高维度向量和onr-hot编码表示的状态和动作或许太过于离散，（这样的坏处是什么？）如何优化现有的动作与状态表示？ 并且，现有的代码open-ptcg框架是否可以丝滑应用于RL，eg，自动识别每个回合的结束？ 3. parameter tuning：现有的(masked)PPO算法是否解决这个问题最合适？并且，采用的各个参数是否合适？</p><h2 id="非马尔可夫性">2.1 非马尔可夫性？</h2><p>先读几篇可能有用的论文： 1.</p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>RL4：策略梯度和策略梯度算法</title>
    <link href="/2025/08/23/RL4/"/>
    <url>/2025/08/23/RL4/</url>
    
    <content type="html"><![CDATA[<h1 id="rl4-策略梯度和一些策略梯度算法">RL4: 策略梯度和一些策略梯度算法</h1><div class="note note-success">            <p>首先介绍几个思想： （1）<strong>策略参数化</strong>：本质上来讲，一个策略是一个从一个特定的状态到一个动作的映射，而策略参数化则是指使用一个函数来表示一个策略，之后通过这个函数在训练的过程中不断调整来优化策略。比如说，我的参数化的策略可能是一个神经网络，根据不同的输入情况输出一系列动作执行的概率，在训练的过程中不断优化当前的方案。 策略参数化表示为 <span class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>)</span>，其中<span class="math inline"><em>θ</em></span>即为用来调整优化策略的参数 （2）</p>          </div><h2 id="策略梯度介绍">1. 策略梯度介绍</h2><p><img src="/RL4pictures/image-36.png" alt="alt text" /> 在之前的RL2中，用到的一些方法都是适用于表格形式的策略，其应用的场景多为状态-动作的数量比较小的情况；而在更多的情况下使用策略梯度是另外一种可行的方案，二者的核心区别为： <img src="/RL4pictures/image-37.png" alt="alt text" /> 对于表格型策略，由于对于每一个状态<span class="math inline"><em>s</em></span>其均最大化了<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>，因此其一定是最优解；而根据参数化策略优化的就不一定了（可能是一个局部的最优解），二者的关系如下：<img src="/RL4pictures/image-38.png" alt="alt text" /> 总体来说，策略优化即为：寻找一个用于衡量策略的优劣的目标函数 <strong><span class="math inline"><em>J</em>(<em>θ</em>)</span></strong> ，然后借助目标函数优化。 因此，对于策略梯度，其关键点就在于以下的两点： （1）如何设计合适的目标函数？ （2）如何进行计算和优化？（设定合适的learning rate 计算梯度，etc.） ### 1.1 目标函数的选择 如图所示，一般有如下的几种选择目标函数的方式，根据不同的任务和算法需求，选取不同的目标函数：<img src="/RL4pictures/image-39.png" alt="alt text" /> <div class="note note-primary">            <p>这几个公式的意义： 1.即为再当前策略下每一个状态的期望价值的按照概率分布的加权求和 2.计算的相当于是整个系统的总奖励值，具体来说：的第二个求和代表的是给定一个状态，在当前策略下各个动作奖励的概率加权和，也就是代表每一个状态所取得的期望奖励；那么前一个求和就是在当前状态分布下每一个状态对应奖励的概率加权和，也就是整个系统的总奖励 3.这是相对于轨迹而言的计算方法，相当于我在当前的策略<span class="math inline"><em>θ</em></span>下，采样一个固定的时间T之后，对于每一个出现的状态-动作序列最终得到的奖励的加权和。<span class="math inline"><em>p</em><sub><em>θ</em></sub></span>即为出现这一串特定状态-动作的各个步骤概率的乘积。 并且，这三个函数都是所有状态的期望价值和，即排除了状态s对结果可能的干扰。</p>          </div> 可以感觉到，这三个基本都是价值函数的不同计算方法，因此策略参数<span class="math inline"><em>θ</em></span>应当让这三个J应当更大。 具体解释一下几个概念： （1）<strong>状态分布</strong>：状态分布指的就是在当前的环境下，各个状态在整个系统中出现的概率分别是多少。对于一个陌生未知的环境<span class="math inline"><em>d</em><sub>0</sub>(<em>s</em>)</span>,其状态分布需要计算和估计，此时又有两种情况如下：<img src="/RL4pictures/image-65.png" alt="alt text" /> 具体解释：策略无关的状态分布指的是，在长时间的实验次数中，每个状态出现的概率与你采用的策略是无关的，因此可以采用这种比较简单的假设； 而当状态分布与策略有关的时候，采用的所谓<strong>稳态状态分布</strong>指的是，在执行相当长时间后，我的各个状态的概率会不再发生明显变化。稳态状态公式也很好理解：求和的三项即为状态s下采用动作a可以转移到<span class="math inline"><em>s</em>′</span>的概率，乘以当前策略下状态s采取策略a的概率，最后再乘以状态s的分布概率。也就是说，把所有状态下可能转移到s’的情况的概率进行求和，即为s’的分布概率。</p><h3 id="计算策略梯度">1.2 计算策略梯度</h3><h4 id="平均轨迹回报目标的策略梯度">1.2.1 平均轨迹回报目标的策略梯度</h4><p>首先来看一下计算轨迹对应的<span class="math inline"><em>J</em>(<em>θ</em>)</span>的策略梯度的方法：<img src="/RL4pictures/image-66.png" alt="alt text" />这个计算看上去很复杂，首先明确几个事情： （1）<span class="math inline"><em>θ</em></span>是一个向量 （2）这里面用到了积分的形式，实际上可以这么理解：比如说你玩游戏，把每一个画面都看作一个状态，把向四面八方任何的移动都看成动作，那么动作和状态空间就是连续的，因此可以理解采集到的轨迹也是连续的，因此使用积分来计算总的reward （3）因此，现在的<span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>τ</em>)</span>是一个概率密度函数（关于<span class="math inline"><em>τ</em></span>），所有求积分即可 由于对于每一个轨迹，其reward<span class="math inline"><em>G</em>(<em>τ</em>)</span>是一个可以计算出的常数，本质上说是与参数<span class="math inline"><em>θ</em></span>无关的；这里注意到<span class="math inline"><em>p</em><sub><em>θ</em></sub>(<em>τ</em>)</span>是一个一系列概率连乘的形式，蓝色的部分就是创造出了一个对数形式的转换，结合链式法则，方便后续的操作。最后又将其写成了期望的形式。 进一步地对于这个梯度进行的展开如下：<img src="/RL4pictures/image-67.png" alt="alt text" /></p><h4 id="平均价值回报和平均奖励回报的策略梯度">1.2.2 平均价值回报和平均奖励回报的策略梯度</h4><p>如图所示<img src="/RL4pictures/image-68.png" alt="alt text" />几点需要说明的是： （1）d(s)可以看作一个在训练过程中，不断优化的“常数”，也就是说随着训练过程会不断变化 （2）recall：<span class="math inline"><em>q</em><sub><em>π</em></sub>(<em>s</em>, <em>a</em>)</span>实际上就是一个奖励值，与具体的<span class="math inline"><em>θ</em></span>其实是无关的！ （3）最下面的<span class="math inline"><em>h</em><sub><em>θ</em></sub>(<em>s</em>, <em>a</em>)</span>指的是我的网络对于状态s下进行动作a的一个“打分”，所以使用softmax函数将其转化为正常的、易于理解的此时策略<span class="math inline"><em>θ</em></span>的采取这个行为的概率（也就是进行归一化处理） 到此为止，介绍了几种不同的策略梯度的计算方法，以此为基础延伸出了一系列近似方法和基于策略梯度的算法，核心来看就是下面的公式： <img src="/RL4pictures/image-69.png" alt="alt text" /></p><h3 id="策略梯度算法">1.3 策略梯度算法</h3><h4 id="reinforce算法">1.3.1 REINFORCE算法</h4><p>这个算法比较简单，核心思想如下图：<img src="/RL4pictures/image-70.png" alt="alt text" /> 也就是说，要想求这个期望值，我们就直接模拟N个轨迹，之后根据蒙特卡洛方法，使用均值估计策略梯度的期望值（公式中的上标n只代表当前轨迹的编号） <div class="note note-success">            <p>对于Reinforce算法一个常用的优化方法是，添加一个baseline，即维护一个所有奖励的平均值为b，之后在上述计算的过程中，用<span class="math inline"><em>G</em>(<em>τ</em>) − <em>b</em></span>代替<span class="math inline"><em>G</em>(<em>τ</em>)</span>，这样可以减少计算结果的方差，如图所示，并且实际上这样不会对系统的梯度值产生变化。 <img src="/RL4pictures/image-71.png" alt="alt text" /></p>          </div> 策略梯度算法是一种<strong>同策略（On-policy）</strong>的算法，在使用中的缺点是由于所有的采样采取的策略相同，因此每次更新的量会很小，参数的下降率会很小，因此单独使用梯度算法会相对很低效。 为了解决这个问题，有几种数学方法可以处理，这里先省略（） 这是对于reinforce的总结：<img src="/RL4pictures/image-78.png" alt="alt text" /></p><h4 id="actor-critic算法">1.3.2 Actor-Critic算法</h4><p>本质上来说，这个算法是把基于策略与基于价值的两种算法结合了起来进行使用。具体来说，按照如下的方式进行实现：<img src="/RL4pictures/image-100.png" alt="alt text" /> 在这个系统中，有两张神经网络 （1）actor网络，这个网络负责“表演”，也就是说，这个网络按照当前的策略在环境中采样 （2）critic网络，这个网络负责“评估”，也就是说，这个网络在当前的策略下，评估当前状态的“价值” actor-critic算法实现过程分为以下几个阶段： （1）首先，随机初始化参数<span class="math inline"><em>θ</em></span>（actor网络的参数）以及critic网络的参数 （2）然后根据当前的<span class="math inline"><em>π</em>(<em>θ</em>)</span>，在环境中进行采样，在这个例子中应该是采样了N个episode （3）在每一步中，Critic网络更新自己的参数，使得其对于Q值的预测与真实值的差距越来越小 （4）之后，利用Critic评估得到的Q值，结合策略梯度算法更新actor网络的参数，再如此循环。</p><h4 id="a2c算法">1.3.3 A2C算法</h4><p>A2C算法（Advantage Actor Critic）是Actor-Critic算法的一个变种（升级）其与前面内容一个基本的区别就是其对于梯度的计算做了一个改进：<img src="/RL4pictures/image-101.png" alt="alt text" />（感觉有点类似于前面对REINFORCE算法改进减掉的一个均值，这里相当于是用当前特定动作的期望reward减去了这个状态下的平均期望reward；好处同样是可以降低方差，并且更加明显增加好动作、减少坏动作的概率（由于这样一来，低于均值的动作对应的参数会减小，反之才会增加） 并且，在具体操作的时候，注意到状态转移的公式<img src="/RL4pictures/image-102.png" alt="alt text" /> 因此，实际上只要维护一个关于V的神经网络来更新各个状态对应的V值就可以了，具体如下：<img src="/RL4pictures/image-103.png" alt="alt text" /> 一个完整的deep A2C的过程如下：<img src="/RL4pictures/image-104.png" alt="alt text" /> 其中，对各个部分梯度的计算根据链式法则蕴含在神经网络各个部分中，一般地最后一步会通过一个softmax进行计算</p><div class="note note-danger">            <p>关于A2C批次更新优化和A3C的内容暂时先略过了</p>          </div><div class="note note-success">            <p>学完了这部分内容，如果向没有学习过这部分知识之前的我介绍什么是策略梯度？或许我会说：这就相当于你先去玩游戏，做出了一系列动作，得到了一系列的奖励（和惩罚）；之后，自然会想要修改策略（的参数），使得好的动作发生概率提升，而不好的动作概率降低。 而在这个基本想法的基础下，策略梯度算法就是让你沿着梯度方向修改这些参数，这样可以最高效地学习。并且，在前面环境已知的情况下，直接根据价值来调整行为显然是容易的，而这里体现的就是一个面对未知情况随机初始化值，然后不断更新的过程。</p>          </div><div class="note note-info">            <p>对这部分内容的补充说明： 1. 对于策略梯度的积分计算的证明是十分复杂的，并且上面的式子也不是非常严谨，在实操中，关键点其实如下所示：<img src="/RL4pictures/image-107.png" alt="alt text" /> 也就是说，我们的reinforce和A2C算法实际上就是用了两种不同的方法来估计这个<span class="math inline"><em>Q</em><sub><em>π</em></sub>(<em>s</em>, <em>a</em>)</span>，前者用实际观测的回报来近似，而后者使用了一个网络来维护这个值。 2. 再详细讲一下A2C中的价值网络：<img src="/RL4pictures/image-108.png" alt="alt text" /> 也就是说，其根据具体的某一个输入的状态会输出这个状态下各个行为的价值。 A2C的核心如下：<img src="/RL4pictures/image-109.png" alt="alt text" /> 具体的训练流程如下：<img src="/RL4pictures/image-110.png" alt="alt text" /></p>          </div>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习，笔记，策略梯度，A2C</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL3：深度学习&amp;DQN</title>
    <link href="/2025/08/13/RL3/"/>
    <url>/2025/08/13/RL3/</url>
    
    <content type="html"><![CDATA[<h1 id="rl3-深度学习dqn">RL3: 深度学习&amp;DQN</h1><div class="note note-success">            <p>这个部分的笔记主要介绍一些深度学习的基础内容，以及其与强化学习结合的DQN方法的例子。由于深度学习不是这个部分的重点，就简单梳理一下框架，具体的各个算法和函数背后的原理就罗列一下，不去深入学习了（）</p>          </div><h2 id="深度学习基础">1. 深度学习基础</h2><h3 id="神经元神经网络神经网络训练">1.1 神经元&amp;神经网络，神经网络训练</h3><p>在神经网络中，“神经元”是其中最基本的组成单位，其基本结构如下所示：<img src="/RL3pictures/image-26.png" alt="alt text" /> 也就是说，输入了一系列有一定权重的量加上偏置值，然后经过激活函数，得到输出。有如下几个概念： （1）<strong>感知机（perceptron）</strong>：我理解感知机是一种单元的、初级的神经元（单元的网络），其包括了神经元的所有特性内容，但是最后的激活函数<span class="math inline"><em>f</em></span>一般是简单的<strong>符号函数</strong> ，即简单的对于内容的正负进行区分。 感知机只能用于处理简单的<strong>线性可分问题</strong> ，由于其激活函数的限制，只能生成一个二分数据的超平面。对于复杂一点的情况（比如<strong>xor</strong>问题），其就无法解决。 （2）<strong>激活函数</strong>：为了解决感知机无法解决的问题，现代神经网络采用了多种多样的激活函数，如图所示，用于不同的场景 <img src="/RL3pictures/image-27.png" alt="alt text" /> <img src="/RL3pictures/image-28.png" alt="alt text" /> 以上介绍了神经网络中神经元的基本结构，下面是一些更加深入的内容 （3）<strong>损失函数</strong>：损失函数是评估训练时神经网络输出的结果和真实值之间差距的函数，是用来衡量训练出的模型好坏的评估指标。根据问题的不同，分别选择不同的损失函数，如图所示 <img src="/RL3pictures/image-29.png" alt="alt text" /> <img src="/RL3pictures/image-30.png" alt="alt text" /></p><div class="note note-primary">            <p>神经网络的两个主要研究对象： <strong>回归问题（regression problem）</strong>：预测一个连续的数值输出，即根据输入的特征预测一个特定的数值。比如说，预测房价、温度，etc <strong>分类问题（classification problem）</strong>：预测一个离散的类别，对于输入的内容将其给出一个特定的分类。比如说，邮件分类、疾病预测，etc</p>          </div><ol start="4" type="1"><li><strong>优化器（optimizer）</strong>：优化器是神经网络中用来更新各个参数以最小化损失函数的算法，常见的方法有梯度下降以及伴随的各种衍生的算法,如图所示<img src="/RL3pictures/image-31.png" alt="alt text" /><img src="/RL3pictures/image-32.png" alt="alt text" /><img src="/RL3pictures/image-33.png" alt="alt text" /></li><li><strong>反向传播(back propagation)</strong>:反向传播是在确定了上面的损失函数和优化方法之后，对于整个神经网络进行更新的过程。 <div class="note note-primary">            <p><img src="/RL3pictures/image-35.png" alt="alt text" /> 以这个简单的例子为例，展示了多层神经网络中，如何利用每两层之间的函数关系与链式法则来计算最终的损失函数对于其中每一个神经元得到的梯度</p>          </div> 在反向传播的过程中，会从输出层向前计算出每一个参数对应的梯度，然后再根据优化器选择的方法依次更新这些参数 <div class="note note-info">            <p>以经典的<strong>手写数字识别（MNIST）</strong> 为例，来分析一下整个神经网络运作的流程： 为了解决这个问题，搭建一个简单的三层的神经网络： （1）输入层：共有28*28=784个神经元，对应每张图片的每一个像素点（灰度），取值在1~255之间，用于输入图片的信息</p><p>（2）隐藏层：隐藏层的神经元个数不定（是一个超参数），假设有128个神经元，那么每一个神经元都会有一系列的参数，接受输入层的784个输入，并且通过线性变换和偏置的叠加得到一个值，接着经过激活之后得到对应的128个输出</p><p>（3）输出层：由于识别的是0~9这些数字，因此输出层就设定为10个神经元，每个都接受隐藏层的128个输出，之后通过神经元内的计算、激活函数激活之后得到输出，每一个输出对应得到结果是相应数字的概率。</p><p>之后，神经网络的整个工作流程如下：</p><p>（0）首先，将所有的图片进行标注之后，分为训练组和测试组，将训练组的数据投喂给模型</p><p>（1）（大多情况下，对于输入的灰度值进行归一化，将每一个值除以255，使得输入的灰度值都在0~1之间）之后进入隐藏层线性计算后，选择激活函数Relu进行激活，得到128个输出（这里使用最简单的函数Relu引入了非线性的量 （2）一张图片进入输出层，同样是先线性计算，之后使用softmax函数得到十个概率值，即为测试得到的输出 （3）对于每一个样例，将得到的预测结果与标签结果相比较，计算损失，接着反向传播计算梯度并且根据优化方法更新参数，完成这个图片的训练 （4）重复步骤（2）和（3），直到所有图片都训练完成</p>          </div></li></ol><h3 id="循环神经网络">1.2 循环神经网络</h3><p>传统的深度学习框架对于一些“一锤子买卖”的任务处理的结果会很好，但是对于一些需要“记忆”的任务，比如说，想让其理解一个电影片段的内容，就会比较困难。因此，具有“记忆”单元的RNN应运而生。 循环神经网络的基础结构如下：<img src="/RL3pictures/image-51.png" alt="alt text" /> 在基本的神经网络之外，有一个循环的loop；事实上，这样的结构可以看成是一系列神经网络的继承，“each passing a message to a successor”,即如下图的结构：<img src="/RL3pictures/image-52.png" alt="alt text" /> 对于这个图，更详细的解释是，每一个神经网络会有一个输出值<span class="math inline"><em>h</em><sub><em>t</em></sub></span>，这个值会作为下一层神经网络的输入之一，并且被给予一定的权重矩阵，与这个时间点的其他输入共同作用，形成下一个时间点的输出<span class="math inline"><em>h</em><sub><em>t</em> + 1</sub></span>；并且，RNN的特点是所有的神经网络的所有参数和权重都是<strong>完全一样</strong>的，这被称为RNN的<strong>权重共享</strong>. <img src="image-54.png" alt="alt text" /> e.g. 一个简单的例子：对于前一步的输出以及这一步的输入，简单使用一个tanh作为激活函数得到进一步的值 <div class="note note-primary">            <p>权重共享的好处是什么？总结下来大致如下： <img src="/RL3pictures/image-53.png" alt="alt text" /></p>          </div> 但是，传统的RNN往往会伴随着梯度爆炸等等的问题，因此大部分RNN取得的相关成果都于LSTM有关。 <div class="note note-warning">            <p>在使用RNN的时候，一个经常出现的问题就是如果相关的内容与你现在需要预测的内容距离差的很远，就会出现失效；比如说，预测一个句子的上下文，如果相关信息就在当前的这句话内，则RNN可以预测成功；然而，如果提供线索的上文距离比较远，RNN便会失效。但是，使用LSTM就不会出现这个问题！ 有关LSTM的内容，可以参考<a href="LSTM%20A%20Search%20Space%20Odyssey.md">论文：LSTM: A Search Space Odyssey</a></p>          </div></p><h3 id="dqn">1.3 DQN</h3>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习，笔记，深度学习，DQN，神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL2：Value estimation</title>
    <link href="/2025/08/07/RL2/"/>
    <url>/2025/08/07/RL2/</url>
    
    <content type="html"><![CDATA[<h1 id="rl2-value-estimation">RL2: Value estimation</h1><div class="note note-success">            <p>在<a href="https://zrwrz.github.io/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/"><strong><u>RL1</u></strong></a>中，介绍了MDP和基于其建模的动态规划方法。然而，这种方法十分依赖于对于整个环境和模型的充分了解；而在更多的无法明确地给出状态转移和奖励函数的情况下，我们就会更加依赖于直接从获得的数据中学习相应的价值与策略。 &gt;在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为无模型的强化学习（model-freereinforcement learning）</p>          </div><h2 id="蒙特卡洛方法">1.蒙特卡洛方法</h2><h3 id="总体思路概述">1.1 总体思路概述</h3><p><strong>蒙特卡洛方法（Monte-Carlo methods，简称 MC）</strong> 在数学上是指一种依赖于大量随机抽样得到数值结果的方法；而在RL中，其思想为使用策略<span class="math inline"><em>π</em></span>从状态<span class="math inline"><em>s</em></span>采样<span class="math inline"><em>N</em></span>个样本，并使用经验均值累计奖励近似期望累计奖励(也就是V)的方法</p><h3 id="具体实现方法">1.2 具体实现方法</h3><p>一般来说，在实现蒙特卡罗方法的时候，我们需要使用固定的策略模拟若干个回合并且相应记录累计奖励，对于没有明确终止条件的情况，一般规定一个<span class="math inline"><em>T</em></span>为应用策略<span class="math inline"><em>π</em></span>采样的时间数。（只有有限MDP可以模拟到结束）之后，通过不断累加总的采样次数<span class="math inline"><em>N</em>(<em>s</em>)</span>(表示从状态s开始采样)和总的累计奖励<span class="math inline"><em>V</em>(<em>s</em>)</span>,当<span class="math inline"><em>N</em>(<em>s</em>)</span>足够大的时候，就可以利用<span class="math inline"><em>V</em>(<em>s</em>)/<em>N</em>(<em>s</em>)</span>来估计状态<span class="math inline"><em>s</em></span>的价值。</p><div class="note note-primary">            <p>1.总结一下，Monte-Carlo就是提供了在对环境完全未知的情况下通过多次模拟算出<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>的方法，这可以 <strong>“帮助你理解，在某一状态下，执行策略的长期回报是什么。这对于强化学习的策略评估非常有用，尤其是在不知道环境动态的情况下。”</strong> 2.在实操的时候，为了节约计算量，直接使用增量的方式进行更新<img src="/RL2pictures/image-11.png" alt="alt text" /> 第二部分的更新公式为后面Q-learning要使用的增量公式，数学推导也很简单，具体作用往下看</p>          </div><p>有些时候，会碰到更加复杂的情况：有可能环境不仅转移概率为止，而且可能是<strong>非平稳环境（Non-Stationary Environment）</strong> ，即环境的转移概率和奖励函数会随时间而改变。在这种时候，按照之前的方法采用蒙特卡洛显然不适用，因此采用以下的<strong>滑动窗口</strong>技术：<img src="/RL2pictures/image-12.png" alt="alt text" /> 在这个时候，仍然像之前一样算出每一次采样之后的V值，然后与先前不同的是，采用一个固定长度的“窗口”进行计算：每次纳入一个新的值的同时删除最老的。在这个时候，由于窗口长度固定，因此增量的改变也变成了乘上固定的系数<span class="math inline"><em>α</em></span>，其大小为窗口长度的倒数。<strong>(和learning rate一点关系都没有！)</strong></p><div class="note note-warning">            <p>那么环境不断变化，窗口什么时候才要停下来呢？gpt这么解释： &gt; 在非平稳环境中，环境是不断变化的，因此滑动窗口和蒙特卡洛方法的 停止条件 和 估计收敛 是相对复杂的。关键在于平衡 适应环境的变化 和 获得稳定估计 之间的关系。在 非平稳环境 中，估计值是不断调整的，因为环境的状态转移和奖励会随时间变化。 有点抽象，后面尽量结合例子理解（大概意思是，会根据环境不断变，就让它一直跑下去就行）</p><p><strong>同时可以比较容易看出，可以使用蒙特卡罗方法进行估计的前提是两个状态之间转移的奖励必须是已知的，否则无法进行计算！</strong></p>          </div><h2 id="时序差分方法">2.时序差分方法</h2><h3 id="总体方法概述">2.1 总体方法概述</h3><p><strong>时序差分方法（ Temporal Difference methods，TD）</strong> 是另外一种模型无关的，直接使用经验学习的方法。时序差分方法的核心公式如下：<img src="/RL2pictures/image-13.png" alt="alt text" /> 在时序差分中，我们通过当前行为带来的即时奖励以及下一个回合的V值与<span class="math inline"><em>γ</em></span>（也就是，对于未来的估计值）来更新当前状态的价值V</p><div class="note note-primary">            <p>这里使用学习率 <strong><span class="math inline"><em>α</em></span></strong> 来控制更新大小的大小，是为了避免产生过度更新的问题，比如更新值过大导致来回震荡，而加上一个系数可以保证平滑更新</p>          </div><h3 id="时序差分-vs-蒙特卡洛">2.2 时序差分 vs 蒙特卡洛</h3><p>（1）从上面的定义就可以看出，时序差分方法在每一次执行动作之后都会利用下一个状态的<span class="math inline"><em>V</em></span>值更新（每一步后可以进行在线学习），而蒙特卡洛则需要跑完一个回合之后才会利用累计奖励对于<span class="math inline"><em>V</em>(<em>s</em>)</span>值进行更新 （2）很容易知道蒙特卡洛方法一定是<strong>无偏估计</strong>，而时序差分方法由于使用的就是下一个状态的猜测值<span class="math inline"><em>V</em>(<em>s</em><sub><em>t</em> + 1</sub>)</span>而不是该策略下的真实值<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em><sub><em>t</em> + 1</sub>)</span>。因此，TD是有偏估计。 （3）时序差分方法对于初始值更加敏感，而蒙特卡洛方法对于初始值不敏感 （4）蒙特卡洛方法得到的结果的方差更大，而时序差分方法得到的结果方差更小 总结下来如图所示：<img src="/RL2pictures/image-14.png" alt="alt text" /></p><h2 id="资格迹方法">3. 资格迹方法</h2><h3 id="总体方法概述-1">3.1 总体方法概述</h3><p>如果说把使用窗口的蒙特卡洛方法中的窗口大小倒数<span class="math inline"><em>α</em></span>看作类似于“学习率”的参数，那么前面三个方法的关系可以这样概括：时序差分是只参考当前下一步的估计方法，资格迹方法是参考下面若干步的多步时序差分方法，而蒙特卡洛方法则是参考未来无限步（直到回合结束）的方法。</p><p>因此，<strong>资格迹方法（Eligibility Traces methods）</strong> 可以看作是前面两种方法的一个平衡，其平衡了方差和偏差的关系，是一种比较折中的方案。依葫芦画瓢得到如下的公式：<img src="/RL2pictures/image-15.png" alt="alt text" /></p><h3 id="具体实现">3.2 具体实现</h3><h4 id="td-lambda-方法">3.2.1 <span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span> 方法</h4><p>所谓的 <strong><span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span>方法</strong>，是指使用一个超参数<span class="math inline"><em>λ</em></span> 进行控制后续使用的各个阶段G的权重，具体如下：<img src="/RL2pictures/image-16.png" alt="alt text" /> 解释一下：<span class="math inline"><em>G</em><sub><em>t</em></sub><sup><em>n</em></sup></span> 指的是（上面所说的）考虑了之后n步具体奖励的资格迹方法；第二个公式实际上是一个加权平均（注意到：<strong><span class="math inline"><em>λ</em></span>+<span class="math inline"><em>λ</em><sup>2</sup></span> +…+<span class="math inline"><em>λ</em><sup><em>n</em></sup></span> =<span class="math inline">$\frac{1}{1 - \lambda}$</span></strong> ）。可以看到，距离现在时间越远的远期奖励权重会越来越小，而参数<span class="math inline"><em>λ</em></span>控制着这个权重衰减的速率。</p><p>这种方法较好地平衡了MC和TD，其不仅会有更快的收敛速度（相比于MC），而且平衡了偏差和方差之间的关系；同时，保留了大量的历史信息的加权结果，使得对于价值的估计更加全面。</p><div class="note note-danger">            <p>下面是两张直观反映<span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span>方法的图片：<img src="/RL2pictures/image-17.png" alt="alt text" /><img src="/RL2pictures/image-18.png" alt="alt text" /> 但是暂时不是很理解具体的意思，不知道这个”后向视角“具体是怎么结合的</p>          </div><h2 id="表格型时序差分方法sarsa-q-learning">4. 表格型时序差分方法：SARSA &amp; Q-learning</h2><h3 id="核心思想">4.1 核心思想</h3><p>强化学习的核心部分便是<strong>策略评估和策略优化</strong> （recall：之前dp的策略迭代和策略评估），具体都在下图中有所指出：<img src="/RL2pictures/image-19.png" alt="alt text" /> 也就是说，策略评估就是估算出<span class="math inline"><em>V</em></span>,(也就是之前几个部分在做的)，之后再根据算出来的<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>，来选择最好的策略（动作）</p><h3 id="sarsa">4.2 SARSA</h3><h4 id="总体方法概述-2">4.2.1 总体方法概述</h4><p>所谓的SARSA，指的是<strong>S</strong>tate-<strong>A</strong>ction-<strong>R</strong>eward-<strong>S</strong>tate-<strong>A</strong>ction。先来看一段伪代码，直观反映了这个方法的运行步骤：<img src="/RL2pictures/image-21.png" alt="alt text" /> 作为背景，首先介绍 <strong><span class="math inline"><em>ϵ</em> − <em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em></span></strong> 方法：这是一种在强化学习中常用的<strong>平衡探索与利用</strong>的手段。在每一个时间步中，agent都会以<span class="math inline"><em>ϵ</em></span>的概率随机选择一个动作（实现对未知的探索），以<span class="math inline">1 − <em>ϵ</em></span>的概率选择当前的最优策略（贪心，实现对现在的利用）<br />接着，基于上面策略评估与策略优化的思想，SARSA分为如下几个步骤实现： <strong>step1：</strong> 初始化参数值，包括初始化各个<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>的值，以及初始化每个步骤用于选择的<span class="math inline"><em>ϵ</em></span>初始值</p><p><strong>step2：</strong> 从初始环境<span class="math inline"><em>s</em><sub>0</sub></span>开始，在每一个状态<span class="math inline"><em>s</em></span>处，按照<span class="math inline"><em>ϵ</em></span>-greedy策略进行选择动作<span class="math inline"><em>a</em><sub><em>t</em></sub></span>（即，可能选择当前的最佳，也有可能随即探索），并且根据这个行为观察奖励<span class="math inline"><em>R</em></span>；随后进入下一个状态<span class="math inline"><em>s</em>′</span>,并且再利用一次<span class="math inline"><em>ϵ</em></span>-greedy策略进行选择动作<span class="math inline"><em>a</em>′</span>，并且读取对应的$Q(s’,a’)的值</p><p><strong>step3：</strong> 利用上途中的公式，更新<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>的值；并且一直重复这个过程，直到Q值收敛或者状态结束</p><h4 id="原理是什么">4.2.2 原理是什么？</h4><p>对于SARSA的原理，可以把它看成是针对当前策略的一个 <strong><span class="math inline"><em>T</em><em>D</em></span>的更新</strong>，因为在更新公式中，可以把 <strong><span class="math inline"><em>R</em> + <em>γ</em><em>Q</em>(<em>s</em>′, <em>a</em>′)</span></strong> 这一项看作是对于当前策略奖励（更）准确的值（因为多向前跑了一项），而 <strong><span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span></strong> 则可以看作是当前状态下对于这样一个行为结果的一个预测，因此对于两者给上一个学习率系数就可以实现学习的过程。</p><div class="note note-primary">            <p>SARSA的一个性质是，随着训练的进行，模型会越来越快抵达最后的“目标”，即最后比较接近的预测值。</p>          </div><h3 id="q-learning">4.3 Q-learning</h3><h4 id="基本方法概述">4.3.1 基本方法概述</h4><p>SARSA是一种典型的 <strong>on-policy</strong> 方法，即，在进行学习过程中使用的策略就是智能体使用的策略；而与之不同的是，Q-learning是一种 <strong>off-policy</strong> 方法，这个的意思是，学习的策略与当前使用的策略不同，而是直接使用当前的最优策略进行优化。 Q-learning的核心是如下的这个公式：<img src="/RL2pictures/image-23.png" alt="alt text" /> 可以注意到，和之前的SARSA不同，这里更新的下一步得到奖励使用的是我认为来到下一个状态之后所取得的最优步骤（即为最大的奖励），其他的结构是一样的。（recall之前的SARSA，区别就是SARSA只会以<span class="math inline"><em>ϵ</em></span>的概率随机选择状态<span class="math inline"><em>s</em>′</span>最优的动作）</p><h4 id="具体实现方法-1">4.3.2 具体实现方法</h4><p>可以把Q-learning看作简单版本的SARSA，其每一步都会默认选择最优的策略（而非按照一定的概率）来更新数值，根据数学的定理其一定会收敛到最后的最优策略Q*</p><div class="note note-primary">            <p>这是一个直观展现Q-learning和SARSA两个不同特点的例子：<img src="/RL2pictures/image-24.png" alt="alt text" /> 由于Q-learning倾向于选择”最佳的“，所以其会贴着悬崖走；而SARSA会按照一个更加探索过的路线行走。</p>          </div><div class="note note-success">            <p>总结一下这个部分的内容：和标题一样，这个部分主要介绍了强化学习中的值估计部分的内容，从一开始的MC和TD以及资格迹是对于V值估计的三个方法，到后面的两个表格式TD对于每一个状态-动作组的Q值的估计，这个部分在RL1的基础上进一步展示了当环境对我们更为陌生的时候应该如何对于各个状态和行为的价值进行估计。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
      <tag>蒙特卡洛</tag>
      
      <tag>时序差分</tag>
      
      <tag>资格迹</tag>
      
      <tag>Q-learning</tag>
      
      <tag>SARSA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL0：基础数学知识</title>
    <link href="/2025/08/06/RL0/"/>
    <url>/2025/08/06/RL0/</url>
    
    <content type="html"><![CDATA[<h1 id="rl0-基础数学知识">RL0： 基础数学知识</h1><h2 id="无偏估计">1.无偏估计</h2><p>在统计学中，无偏估计指的是一个估计量的期望值等于其真实值，即 <strong><span class="math inline"><em>E</em>[<em>θ̂</em>] = <em>θ</em></span></strong> 在强化学习的状态估计里面，无偏估计即为我们估计到的<span class="math inline"><em>V</em>(<em>s</em>)</span>与其真实值相同的情况</p><h2 id="梯度">2.梯度</h2><p>recall：对于一个多元函数，其梯度是一个<strong>向量</strong>，为该函数对于每一个变量求偏导数的结果，即为 <span class="math inline">$\nabla f(x_1, x_2, \dots, x_n) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$</span></p><h2 id="悔值">3.悔值</h2><p>对于一个强化学习进行的若干动作来说，其（累计）的悔值即为：<span class="math inline">$R(T) = \sum_{t=1}^{T} \left( \text{损失}(\text{最优动作}) - \text{损失}(\text{选择的动作}) \right)$</span> 也就是说，即为累计的你的决策和理想的最优动作的差距，也就是你在过程中犯的“错误”的累计</p><h2 id="对数极大似然估计">4.（对数）极大似然估计：</h2><p>首先回忆一下<strong>极大似然估计（MLE）</strong>：<img src="image-77.png" alt="alt text" /> 也就是说，对于某个与参数组合<span class="math inline"><em>θ</em></span>有关的概率分布，采样到了一组数值<span class="math inline"><em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em></sub></span>，那么，我们的目标是找到使得观测到这组<span class="math inline"><em>x</em><sub><em>i</em></sub></span>的概率最大的参数<span class="math inline"><em>θ</em></span>，也就是说确定观测到这组数据最合理的参数<span class="math inline"><em>θ</em></span>。对数极大似然估计也就是在碰到连乘的形式的时候，将其转化为对数方便求导计算</p><h2 id="张量">5.张量</h2><p>张量是对于向量 矩阵的一个扩充定义；一维张量是向量，二维是矩阵，张量可以表示三位及以上的数据。</p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RL1：强化学习基本概念，马尔科夫决策过程，DP</title>
    <link href="/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/"/>
    <url>/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/</url>
    
    <content type="html"><![CDATA[<h2 id="rl1强化学习基本概念马尔科夫决策过程dp"># RL1：强化学习基本概念，马尔科夫决策过程，DP</h2><h2 id="基本定义">1. 基本定义</h2><ul><li>强化学习是机器学习的一个分支，专注于让智能体（Agent）通过与环境的交互学习最优策略，以最大化累积奖励。其核心思想是试错学习（Trial-and-Error），类似于人类或动物通过经验改进行为的过程。</li></ul><p>强化学习的主体为<strong>智能体(agent)</strong>，即为对环境做出感知、决策、行动的对象 <em>（e.g. 玩游戏时的角色&amp;自动驾驶的汽车）</em>；<strong>环境(environment)</strong> 是与智能体交互的一切外界规则与机理 <em>（即为智能体所处的“世界”）</em>，</p><div class="note note-warning">            <h4 id="q-价值学习和策略学习分别是什么">Q: 价值学习和策略学习分别是什么？</h4><p>A:（以后慢慢补充）</p>          </div><hr /><h2 id="马尔科夫决策过程markov-decision-processmdp">2. 马尔科夫决策过程(Markov decision process，MDP)</h2><h3 id="mdp基本五要素">2.1 MDP基本五要素</h3><p>MDP是可以通过强化学习解决问题的基础建模，其由以下的五个元素组成𝑀 =&lt; 𝑆,𝐴,𝑃,𝑅,𝛾 &gt;</p><ul><li>状态（State）集合 <strong>S</strong>: 状态指的是当前时刻，对于整个环境一帧画面的概括 <em>(e.g.下棋一个时刻棋盘所有棋子的位置&amp;玩游戏时某时刻的一帧画面)</em></li><li>动作（Action）集合 <strong>A</strong>: （我认为）<strong>动作必须依赖于特定的状态</strong>，动作集合是agent在当前状态下可能做出的所有决策</li><li>状态转移函数<strong>P</strong>: 状态转移取决于当前的状态和行为，并且具有随机性，概率用<strong>状态转移函数</strong> 𝑝(𝑠’|𝑠,𝑎)进行衡量，指的是agent在当前状态s采取行动a转移到状态s’的概率。</li></ul><div class="note note-warning">            <p>要注意状态转移具有<strong>随机性</strong>，由于环境的不确定性和干扰，当前agent即使在给定时刻采取确切的行为，也无法确定下面的状态。下面是一个通俗的例子<img src="/RL1pictures/image.png" /></p>          </div><ul><li>奖励函数<strong>R</strong>: 人为设置的奖励对于训练模型很重要，通常来说需要设置一系列的<strong>r(s,a,s’)</strong> <em>(从状态s采取行为a转移到状态s’的奖励)</em> 给智能体，以达到最终的训练目的；有时奖励会退化为r(s,a) <div class="note note-primary">            <h4 id="r_t与rsa的区别"><span class="math inline"><em>R</em><sub><em>t</em></sub></span>与<span class="math inline"><em>r</em>(<em>s</em>, <em>a</em>)</span>的区别：</h4><p>依旧让gpt来说： &gt; <span class="math inline"><em>r</em>(<em>s</em>, <em>a</em>)</span>: &gt; 表示 <strong>某个状态下，执行某个动作后得到的即时奖励</strong>。 它更像是一个 <strong>奖励函数</strong>，给定状态 <span class="math inline"><em>s</em></span> 和动作<span class="math inline"><em>a</em></span>，返回一个奖励值。 它是环境对动作的即时反馈，通常与时间步无关。</p><blockquote><p><span class="math inline"><em>r</em><sub><em>t</em></sub></span> : 表示在 <strong>特定时间步 <span class="math inline"><em>t</em></span></strong> 下，智能体执行某个动作后得到的即时奖励。 它是 <strong>即时奖励的时刻标记</strong>，指在时刻<span class="math inline"><em>t</em></span> 上的反馈。 这个奖励可能与某个特定的状态或动作有关，通常用于标识当前时间步的奖励。 所以说，主要就是衡量的参数不同（状态、行为 or 时间）</p></blockquote>          </div></li></ul><div class="note note-danger">            <p>缺一个MDP矩阵形式的解，但感觉没啥用，以后再补充</p>          </div><ul><li>折扣因子<strong>𝛾</strong>: 一个介于0和1之间的参数，由于（人之常情）需要多关注眼前立刻获得的回报，因此对于未来若干步之后才能取得的好处就要设置折扣因子来减小权重</li></ul><h3 id="回报价值函数策略">2.2 回报，价值函数，策略</h3><p>首先说明<strong>回合(episode)</strong> 的概念：一个“回合”指的是智能体从初始状态与环境交互直到terminal state的过程，就是 <strong>一局游戏，一次任务从开始到执行结束</strong>.</p><ol type="1"><li><p><strong>回报（reward）</strong>: 回报 <strong><span class="math inline"><em>G</em><sub><em>t</em></sub></span></strong> 实际上是一个统计量，指的是一个回合后（一局游戏结束后），由于所有经过的状态、行为和每一步的奖励都确定了，<span class="math inline"><em>G</em><sub><em>t</em></sub></span> 统计了整个回合（或者有限视野内）从t时刻开始所有奖励的累计值，即为<img src="/RL1pictures/image-2.png" /> 因此，回报是一个确定的值</p></li><li><p><strong>价值函数</strong>: 一个状态的期望回报即为这个状态的价值；价值函数 <strong><span class="math inline"><em>V</em>(<em>s</em>)</span></strong> 输入一个状态，输出这个状态的期望回报；价值函数可以写为 <span class="math inline"><em>V</em>(<em>s</em>) = <em>E</em>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub> = <em>s</em>]</span>。进一步地，将其展开得到：<img src="/RL1pictures/image-3.png" alt="alt text" /></p></li></ol><div class="note note-primary">            <p>对于价值函数的定义式，在引入策略 <span class="math inline"><em>π</em></span>之前，感觉有些抽象，因为状态转移之间没有规律可寻，gpt这么说： &gt; 在 强化学习 或 马尔可夫决策过程（MDP）的背景下，价值函数 𝑉(𝑠)主要用来评估每个状态的“好坏”。如果没有显式的策略𝜋，通常假设状态值函数 V(s) 是在某个<strong>默认策略</strong>下定义的，或者更常见的情况是我们关心的是<strong>最优策略</strong>。</p><p>因此，我觉得V是用来衡量状态“好坏”的抽象概念，具体要结合策略才有实际作用</p>          </div><ol start="3" type="1"><li><strong>策略(policy), <span class="math inline"><em>π</em></span></strong> 策略是智能体通过判断环境和状态，来决定下一步动作的函数。通常来说，策略分为<strong>确定性策略</strong>（一个状态对应唯一的动作：比如说，走迷宫设定策略为一直往前冲，除非看到墙就直接右拐）和<strong>随机性策略</strong>。在机器学习中，我们主要讨论随机性策略。 在随机性策略中,核心内容为概率密度函数 <strong><span class="math inline"><em>π</em>(<em>a</em>|<em>s</em>)</span></strong> ，其得到的结果为一个概率分布，为在当前策略&amp;状态s下，采取动作a的概率，即 <span class="math inline"><em>P</em>(<em>a</em><sub><em>t</em></sub> = <em>t</em>|<em>s</em><sub><em>t</em></sub> = <em>s</em>)</span>。</li></ol><div class="note note-primary">            <p><strong>采样(sample)</strong> 和 <strong>轨迹(trajectory)</strong> 是另外两个比较重要的概念。gpt这样告诉我： &gt; 在 强化学习 中，采样（sample）指的是从环境中收集单一数据点或一小段数据。通常来说，采样是从当前状态中通过采取一个动作获得的反馈信息。这些数据（状态、动作、奖励、下一个状态）通常是通过与环境交互而获得的一个时间步的数据点。每次从环境中收集到的一组（状态、动作、奖励、下一个状态）就是一个 采样。 轨迹（trajectory）通常指的是智能体从某一初始状态出发，通过多次采样所得到的一系列数据点。它是智能体在一段时间内与环境交互的整个过程。</p>          </div><h3 id="v与q-bellman">2.3 V与Q, Bellman</h3><ol start="4" type="1"><li><strong>MDP的价值和回报</strong>：结合上面给出的策略概念，在使用策略<span class="math inline"><em>π</em></span>的前提下，我们很容易得到在相应策略下一个<strong>状态价值函数</strong> <span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>的表达式 <img src="/RL1pictures/image-4.png" alt="alt text" /> 进一步地，定义<strong>价值-行为函数</strong> <span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>，指的是agent在采用策略<span class="math inline"><em>π</em></span>的大方向下，在当前状态s已经采取特定行为a之后，期望得到的总回报。</li></ol><div class="note note-primary">            <h3 id="v与q的关系">V与Q的关系</h3><p>根据上述的定义，可以比较容易得到V与Q之间的计算关系<img src="/RL1pictures/image-5.png" alt="alt text" /> 通俗地说，在给定的策略下，<strong>V为给定状态s的期望价值，Q为给定状态s和行为a的期望价值</strong>；因此V即为<span class="math inline"><em>π</em></span>的概率分布下各个行为期望价值的加权和。 从另一个角度看：<img src="/RL1pictures/image-6.png" alt="alt text" /> 就是拆一步当前行为、状态的即时奖励，再加上当前(s,a)转移到各个新状态的V回报的加权和。（当然还有<span class="math inline"><em>γ</em></span>） <strong>简单来说，V和Q都是回报，只是涉及的变量不同</strong></p>          </div><p>5.<strong>贝尔曼期望方程，贝尔曼最优方程：策略评估与策略优化</strong> （1）贝尔曼期望方程：<img src="/RL1pictures/image-7.png" alt="alt text" /> 数学上来看，贝尔曼期望方程就是根据上面V与Q的关系转化为只有V和Q的形式。我理解其本质是一个“向后迭代一步”的方程，其<strong>将最后的回报转化为仅仅与下一步所有回报值的加权平均</strong>，同时也方便迭代递归求值 （2）贝尔曼最优方程：<img src="/RL1pictures/image-8.png" alt="alt text" /> 如果说贝尔曼期望方程是针对当前的策略计算值，那么贝尔曼最优方程是仅仅关注最优值，即在迭代过程中，不关心策略，而是每一步贪心，选择能让下一步回报最大的动作。</p><hr /><h2 id="动态规划dynamic-programming">3. 动态规划(dynamic programming)</h2><h3 id="introduction">3.1 introduction</h3><p>在RL领域，动态规划主要用来解决一类<strong>已知环境模型</strong>的问题，结合上述的贝尔曼方程与DP的将全局寻求最优转化为每一步寻求为最优的思想解决问题。（后面会很容易看出为什么要这样） 首先定义一系列符号： 状态集合表示为 {0（终止状态）,1,2…n}，每个状态对应的可选动作集为 <strong><span class="math inline"><em>A</em>(<em>i</em>)</span></strong> <strong><span class="math inline"><em>p</em><sub><em>i</em>, <em>j</em></sub>(<em>a</em>)</span></strong> 表示在状态i,采取行动a转移到状态j的概率，<strong><span class="math inline"><em>r</em><sub><em>i</em>, <em>j</em></sub>(<em>a</em>)</span></strong> 表示在状态i,采取行动a转移到状态j的奖励，（前面还有k步后的折扣因子<span class="math inline"><em>γ</em><sup><em>k</em></sup></span>）</p><p>动态规划期望得到一个最优的策略，即在每一个状态<span class="math inline"><em>i</em></span>下，都能给出一个当下最优的动作<span class="math inline"><em>μ</em>(<em>i</em>) ∈ <em>A</em>(<em>i</em>)</span></p><h3 id="基于dp的强化学习算法bellman方程的应用">3.2 基于dp的强化学习算法————Bellman方程的应用</h3><h4 id="策略迭代policy-iteration">3.2.1 策略迭代(policy iteration)</h4><p>对于策略迭代，一般分为如下的两个步骤交替进行<img src="/RL1pictures/image-9.png" alt="alt text" /></p><p>第一步的 <strong>策略评估</strong> 中，采取当前的策略，一般会为所有的状态定义一个（随机的）初始V值，之后不断使用Bellman期望方程进行迭代，直到收敛为止，以得到各个状态真实的奖励V值。 <div class="note note-danger">            <p>初始的策略应该可以完全随机设置；初始的V值感觉也可以随机设置，是否有影响存疑（我觉得无所谓）；同时，强大的<a href="https://baike.baidu.com/item/%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86/9492042"><strong><u>Banach不动点定理</u></strong></a>保证一定可以得到最终的收敛解决；如下的简单推导确保优化结果一定越来越好<img src="/RL1pictures/image-10.png" alt="alt text" /></p>          </div> 得到所有的V值之后，可以相应对于每一个状态s得到所有动作a的Q值（也是根据Bellman方程的公式），之后，进入<strong>策略优化</strong> 阶段，我们选取 <strong><span class="math inline">$\pi(s)=\arg\max\limits_a Q(s, a)$</span></strong> ,即直接用得到Q最大的动作a来作为新的策略 之后不断重复这两步，计算新的V Q，更新策略…… 直到策略收敛不再改变为止。</p><p><strong>更简单地说，贝尔曼方程就是用来求出V值，然后根据V求出所有的Q值，最后根据Q值来更新策略</strong></p><div class="note note-danger">            <p>初学时陷入了一个误区：既然算出V后可以得到获得最好Q的行为a，那为什么不直接选择这个行为作为最佳策略？ 其实也很简单解决，因为这只是目前<span class="math inline"><em>π</em></span>看到的所谓好行为，但对于别的策略并非最优</p>          </div><h4 id="价值迭代value-iteration">3.2.2 价值迭代（value iteration）</h4><p>核心表达式为 <strong><span class="math inline">$V(s) \leftarrow \max\limits_a \left[ r(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right]$</span></strong> ，通俗说，策略迭代需要计算所有行为得到的奖励再加权相加作为当前状态的V，而价值迭代直接把奖励低的pass掉，直接把奖励最高的按100%权重给当前的状态。 就这样，得到新的一轮V，重复这样的方法直到得到的V收敛即为最终确切的状态，然后再根据 <strong><span class="math inline">$\pi^*(s) = \arg\max\limits_a \left[ r(s, a) + \gamma \sum_{s'} P(s' | s, a) V^*(s') \right]$</span></strong> 反推出策略即可 <div class="note note-success">            <p>总结一下Bellman方程与DP: 结合具体的例子看Bellman方程比较好理解，期望方程就是把所有对应的<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>加权得到新的V值，而最优方程就是把结果最好的<span class="math inline"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span>直接作为新的V值；第一种方法对应不断迭代得到收敛的V值再进行优化的策略迭代算法，而由于这个阶段的巨大计算量，价值迭代则直接用最优的替代来节约计算时间。 然而，由于DP算法对于解决问题的要求很苛刻（要求所有的状态转移概率和获得奖励值全部已知），而大多数问题都显然比这个复杂，因此局限性还是很大的。</p>          </div></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
      <tag>马尔科夫决策过程</tag>
      
      <tag>动态规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL home</title>
    <link href="/2025/08/05/RL-notes/"/>
    <url>/2025/08/05/RL-notes/</url>
    
    <content type="html"><![CDATA[<h1 id="about-rl">About RL</h1><p>暂定用来记录一些之前的笔记和后面自学的强化学习相关的内容。</p><h2 id="references">References</h2><p>1.王树森 深度强化学习<br />2. 多智能体强化学习短学期ppt 3. <a href="https://spinningup.openai.com/en/latest/">Spinning Up</a>（这是有用的学习资料）</p><div class="note note-success">            <h3 id="目录">目录</h3><p><a href="/RL1：强化学习基本概念，马尔科夫决策过程，DP/"><u>RL1：强化学习基本概念，马尔科夫决策过程，DP</u></a> <a href="/RL2：Value-estimation/"><u>RL2：Value estimation</u></a> <a href="/RL3：深度学习&amp;DQN/"><u>RL3：深度学习&amp;DQN</u></a> <a href="/RL4：策略梯度和策略优化/"><u>RL4：策略梯度和策略优化</u></a></p>          </div><p>对于整个RL的结构总结一下：我们强化学习的最终目标是要求出一个agent采取的最优策略，那么有两个大方向：<strong>优化价值函数</strong>和<strong>优化策略</strong>。优化价值函数相当于我们先找到在每一个状态下进行什么样的动作可以取得最好的奖励<span class="math inline"><em>Q</em><sup>*</sup></span>，接着让我们的智能体贪心地向着这个目标前进就可以了。因此，基于价值函数的方法一般得到的都是<strong>确定性的策略</strong>，即使偶尔采用<span class="math inline"><em>ϵ</em> − <em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em></span>方法，但大方向就是得到确定性的策略。 然而，在现实中面临更复杂的问题时，我们往往需要的是随机策略；同时，对于动作空间和状态空间非离散的情况，此时采用之前的迭代得到最大价值的方法也行不通了，此时解决这类问题就需要<strong>基于策略梯度</strong>的方法了。两种路径各有各的优点，需要结合具体的问题进行分析。 这样的关系，可以简单用下图总结：<img src="image-76.png" alt="alt text" /></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo 常用命令&amp;指南</title>
    <link href="/2025/08/04/hello-world/"/>
    <url>/2025/08/04/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
