<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>ptcg</title>
    <link href="/2025/08/31/ptcg/"/>
    <url>/2025/08/31/ptcg/</url>
    
    <content type="html"><![CDATA[<h2 id="关于dataptcg下各文件的用途和关系">1. 关于data/ptcg下各文件的用途和关系</h2><h3 id="agent-workflow-memory不是这个项目的">1.1 agent-workflow-memory（不是这个项目的？）</h3><p>AWM(代理工作流) 是一种预先定义或者存储的agent的“工作蓝图”，其可以看成是agent的“记忆”，用于指导LLM完成各种复杂的任务。<img src="image-40.png" alt="alt text" /> 该父文件夹下的 mind2web以及webarena应该都是附带的基准测试文件</p><h3 id="claudeplayspokemonstarter">1.2 ClaudePlaysPokemonStarter</h3><p>这是一个用于将Claude AI用于play pokemon的红方的智能体系统</p><h3 id="open-spiel">1.3 open spiel</h3><p>“OpenSpiel 是 DeepMind 开发的一个专注于游戏强化学习研究的开源框架。它提供了一个丰富的游戏环境集合和算法实现，支持多种类型的游戏和研究场景。” 可以理解为是游戏强化学习的一个平台</p><h3 id="open-ptcg">1.4 open-ptcg</h3><p>应该是主要的工程文件，里面除了包含了之前的整个ptcg的信息之外，还有一些额外的强化学习相关的内容。其中： #### 1.4.1 eval文件夹 eval文件夹中导入了对于训练好的模型进行评估的手段，其中各个文件的作用分别为： ##### (1) eval_sb3_ppo.py 这个文件是用来在训练的过程中定性观察训练出的agent在过程中采取的策略的文件，其对于单个环境进行可视化的演示，方便逐步调试以及观察游戏进程 ##### (2) eval_sb3.py 这个文件用于加载已经训练好的模型，并且评估模型在指定数量的episode中的表现，并且输出得到的奖励结果 ##### (3) eval_all.py 这是一个全方位的评估文件，可以用于多种智能体之间的对比和对战，其可以：<img src="image-41.png" alt="alt text" />同时，创建出一个N*N的不同agent的对战胜利矩阵，以及跑完之后完整的wandb的实验记录和结果分析 ##### (4) evaluate.py 这个文件实现了一个交互式的游戏演示系统，其可以让你实现与AI对战或者实时观察两个智能体之间的战斗过程，是一个与前端相结合的交互式系统</p><h4 id="logsmodels-文件夹">1.4.2 logs&amp;models 文件夹</h4><p>这两个文件夹应该是记录之前（以及之后）训练结果以及对战的各条log和记录</p><h4 id="关于核心的代码架构">1.4.3 关于核心的代码架构</h4><h5 id="关于action-space的定义">（1）关于action space的定义</h5>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>RL4：策略梯度和策略优化</title>
    <link href="/2025/08/23/RL4/"/>
    <url>/2025/08/23/RL4/</url>
    
    <content type="html"><![CDATA[<h1 id="rl4-策略梯度和策略优化">RL4: 策略梯度和策略优化</h1><div class="note note-success">            <p>首先介绍几个思想： （1）<strong>策略参数化</strong>：本质上来讲，一个策略是一个从一个特定的状态到一个动作的映射，而策略参数化则是指使用一个函数来表示一个策略，之后通过这个函数在训练的过程中不断调整来优化策略。比如说，我的参数化的策略可能是一个神经网络，根据不同的输入情况输出一系列动作执行的概率，在训练的过程中不断优化当前的方案。 策略参数化表示为 <span class="math inline"><em>π</em><sub><em>θ</em></sub>(<em>a</em>|<em>s</em>)</span>，其中<span class="math inline"><em>θ</em></span>即为用来调整优化策略的参数 （2）</p>          </div><h2 id="策略梯度介绍">1. 策略梯度介绍</h2><p><img src="image-36.png" alt="alt text" /> 在之前的RL2中，用到的一些方法都是适用于表格形式的策略，其应用的场景多为状态-动作的数量比较小的情况；而在更多的情况下使用策略梯度是另外一种可行的方案，二者的核心区别为： <img src="image-37.png" alt="alt text" /> 对于表格型策略，由于对于每一个状态<span class="math inline"><em>s</em></span>其均最大化了<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>，因此其一定是最优解；而根据参数化策略优化的就不一定了（可能是一个局部的最优解），二者的关系如下：<img src="image-38.png" alt="alt text" /> 总体来说，策略优化即为：寻找一个用于衡量策略的优劣的目标函数 <strong><span class="math inline"><em>J</em>(<em>θ</em>)</span></strong> ，然后借助目标函数优化。 因此，对于策略梯度，其关键点就在于以下的两点： （1）如何设计合适的目标函数？ （2）如何进行计算和优化？（设定合适的learning rate 计算梯度，etc.） ### 1.1 目标函数的选择 如图所示，一般有如下的几种选择目标函数的方式，根据不同的任务和算法需求，选取不同的目标函数：<img src="image-39.png" alt="alt text" /></p>]]></content>
    
    
    
  </entry>
  
  
  
  <entry>
    <title>RL3：深度学习&amp;DQN</title>
    <link href="/2025/08/13/RL3/"/>
    <url>/2025/08/13/RL3/</url>
    
    <content type="html"><![CDATA[<h1 id="rl3-深度学习dqn">RL3: 深度学习&amp;DQN</h1><div class="note note-success">            <p>这个部分的笔记主要介绍一些深度学习的基础内容，以及其与强化学习结合的DQN方法的例子。由于深度学习不是这个部分的重点，就简单梳理一下框架，具体的各个算法和函数背后的原理就罗列一下，不去深入学习了（）</p>          </div><h2 id="深度学习基础">1. 深度学习基础</h2><h3 id="神经元神经网络神经网络训练">1.1 神经元&amp;神经网络，神经网络训练</h3><p>在神经网络中，“神经元”是其中最基本的组成单位，其基本结构如下所示：<img src="/RL3pictures/image-26.png" alt="alt text" /> 也就是说，输入了一系列有一定权重的量加上偏置值，然后经过激活函数，得到输出。有如下几个概念： （1）<strong>感知机（perceptron）</strong>：我理解感知机是一种单元的、初级的神经元（单元的网络），其包括了神经元的所有特性内容，但是最后的激活函数<span class="math inline"><em>f</em></span>一般是简单的<strong>符号函数</strong> ，即简单的对于内容的正负进行区分。 感知机只能用于处理简单的<strong>线性可分问题</strong> ，由于其激活函数的限制，只能生成一个二分数据的超平面。对于复杂一点的情况（比如<strong>xor</strong>问题），其就无法解决。 （2）<strong>激活函数</strong>：为了解决感知机无法解决的问题，现代神经网络采用了多种多样的激活函数，如图所示，用于不同的场景 <img src="/RL3pictures/image-27.png" alt="alt text" /> <img src="/RL3pictures/image-28.png" alt="alt text" /> 以上介绍了神经网络中神经元的基本结构，下面是一些更加深入的内容 （3）<strong>损失函数</strong>：损失函数是评估训练时神经网络输出的结果和真实值之间差距的函数，是用来衡量训练出的模型好坏的评估指标。根据问题的不同，分别选择不同的损失函数，如图所示 <img src="/RL3pictures/image-29.png" alt="alt text" /> <img src="/RL3pictures/image-30.png" alt="alt text" /></p><div class="note note-primary">            <p>神经网络的两个主要研究对象： <strong>回归问题（regression problem）</strong>：预测一个连续的数值输出，即根据输入的特征预测一个特定的数值。比如说，预测房价、温度，etc <strong>分类问题（classification problem）</strong>：预测一个离散的类别，对于输入的内容将其给出一个特定的分类。比如说，邮件分类、疾病预测，etc</p>          </div><ol start="4" type="1"><li><strong>优化器（optimizer）</strong>：优化器是神经网络中用来更新各个参数以最小化损失函数的算法，常见的方法有梯度下降以及伴随的各种衍生的算法,如图所示<img src="/RL3pictures/image-31.png" alt="alt text" /><img src="/RL3pictures/image-32.png" alt="alt text" /><img src="/RL3pictures/image-33.png" alt="alt text" /></li><li><strong>反向传播(back propagation)</strong>:反向传播是在确定了上面的损失函数和优化方法之后，对于整个神经网络进行更新的过程。 <div class="note note-primary">            <p><img src="/RL3pictures/image-35.png" alt="alt text" /> 以这个简单的例子为例，展示了多层神经网络中，如何利用每两层之间的函数关系与链式法则来计算最终的损失函数对于其中每一个神经元得到的梯度</p>          </div> 在反向传播的过程中，会从输出层向前计算出每一个参数对应的梯度，然后再根据优化器选择的方法依次更新这些参数 <div class="note note-info">            <p>以经典的<strong>手写数字识别（MNIST）</strong> 为例，来分析一下整个神经网络运作的流程： 为了解决这个问题，搭建一个简单的三层的神经网络： （1）输入层：共有28*28=784个神经元，对应每张图片的每一个像素点（灰度），取值在1~255之间，用于输入图片的信息</p><p>（2）隐藏层：隐藏层的神经元个数不定（是一个超参数），假设有128个神经元，那么每一个神经元都会有一系列的参数，接受输入层的784个输入，并且通过线性变换和偏置的叠加得到一个值，接着经过激活之后得到对应的128个输出</p><p>（3）输出层：由于识别的是0~9这些数字，因此输出层就设定为10个神经元，每个都接受隐藏层的128个输出，之后通过神经元内的计算、激活函数激活之后得到输出，每一个输出对应得到结果是相应数字的概率。</p><p>之后，神经网络的整个工作流程如下：</p><p>（0）首先，将所有的图片进行标注之后，分为训练组和测试组，将训练组的数据投喂给模型</p><p>（1）（大多情况下，对于输入的灰度值进行归一化，将每一个值除以255，使得输入的灰度值都在0~1之间）之后进入隐藏层线性计算后，选择激活函数Relu进行激活，得到128个输出（这里使用最简单的函数Relu引入了非线性的量 （2）一张图片进入输出层，同样是先线性计算，之后使用softmax函数得到十个概率值，即为测试得到的输出 （3）对于每一个样例，将得到的预测结果与标签结果相比较，计算损失，接着反向传播计算梯度并且根据优化方法更新参数，完成这个图片的训练 （4）重复步骤（2）和（3），直到所有图片都训练完成</p>          </div></li></ol><h3 id="深度学习rnn和cnn">1.2 深度学习————RNN和CNN</h3><h4 id="卷积神经网络">1.2.1 卷积神经网络</h4>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习，笔记，深度学习，DQN，神经网络</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL2：Value estimation</title>
    <link href="/2025/08/07/RL2/"/>
    <url>/2025/08/07/RL2/</url>
    
    <content type="html"><![CDATA[<h1 id="rl2-value-estimation">RL2: Value estimation</h1><div class="note note-success">            <p>在<a href="https://zrwrz.github.io/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/"><strong><u>RL1</u></strong></a>中，介绍了MDP和基于其建模的动态规划方法。然而，这种方法十分依赖于对于整个环境和模型的充分了解；而在更多的无法明确地给出状态转移和奖励函数的情况下，我们就会更加依赖于直接从获得的数据中学习相应的价值与策略。 &gt;在这种情况下，智能体只能和环境进行交互，通过采样到的数据来学习，这类学习方法统称为无模型的强化学习（model-freereinforcement learning）</p>          </div><h2 id="蒙特卡洛方法">1.蒙特卡洛方法</h2><h3 id="总体思路概述">1.1 总体思路概述</h3><p><strong>蒙特卡洛方法（Monte-Carlo methods，简称 MC）</strong> 在数学上是指一种依赖于大量随机抽样得到数值结果的方法；而在RL中，其思想为使用策略<span class="math inline"><em>π</em></span>从状态<span class="math inline"><em>s</em></span>采样<span class="math inline"><em>N</em></span>个样本，并使用经验均值累计奖励近似期望累计奖励(也就是V)的方法</p><h3 id="具体实现方法">1.2 具体实现方法</h3><p>一般来说，在实现蒙特卡罗方法的时候，我们需要使用固定的策略模拟若干个回合并且相应记录累计奖励，对于没有明确终止条件的情况，一般规定一个<span class="math inline"><em>T</em></span>为应用策略<span class="math inline"><em>π</em></span>采样的时间数。（只有有限MDP可以模拟到结束）之后，通过不断累加总的采样次数<span class="math inline"><em>N</em>(<em>s</em>)</span>(表示从状态s开始采样)和总的累计奖励<span class="math inline"><em>V</em>(<em>s</em>)</span>,当<span class="math inline"><em>N</em>(<em>s</em>)</span>足够大的时候，就可以利用<span class="math inline"><em>V</em>(<em>s</em>)/<em>N</em>(<em>s</em>)</span>来估计状态<span class="math inline"><em>s</em></span>的价值。</p><div class="note note-primary">            <p>1.总结一下，Monte-Carlo就是提供了在对环境完全未知的情况下通过多次模拟算出<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>的方法，这可以 <strong>“帮助你理解，在某一状态下，执行策略的长期回报是什么。这对于强化学习的策略评估非常有用，尤其是在不知道环境动态的情况下。”</strong> 2.在实操的时候，为了节约计算量，直接使用增量的方式进行更新<img src="/RL2pictures/image-11.png" alt="alt text" /> 第二部分的更新公式为后面Q-learning要使用的增量公式，数学推导也很简单，具体作用往下看</p>          </div><p>有些时候，会碰到更加复杂的情况：有可能环境不仅转移概率为止，而且可能是<strong>非平稳环境（Non-Stationary Environment）</strong> ，即环境的转移概率和奖励函数会随时间而改变。在这种时候，按照之前的方法采用蒙特卡洛显然不适用，因此采用以下的<strong>滑动窗口</strong>技术：<img src="/RL2pictures/image-12.png" alt="alt text" /> 在这个时候，仍然像之前一样算出每一次采样之后的V值，然后与先前不同的是，采用一个固定长度的“窗口”进行计算：每次纳入一个新的值的同时删除最老的。在这个时候，由于窗口长度固定，因此增量的改变也变成了乘上固定的系数<span class="math inline"><em>α</em></span>，其大小为窗口长度的倒数。<strong>(和learning rate一点关系都没有！)</strong></p><div class="note note-warning">            <p>那么环境不断变化，窗口什么时候才要停下来呢？gpt这么解释： &gt; 在非平稳环境中，环境是不断变化的，因此滑动窗口和蒙特卡洛方法的 停止条件 和 估计收敛 是相对复杂的。关键在于平衡 适应环境的变化 和 获得稳定估计 之间的关系。在 非平稳环境 中，估计值是不断调整的，因为环境的状态转移和奖励会随时间变化。 有点抽象，后面尽量结合例子理解（大概意思是，会根据环境不断变，就让它一直跑下去就行）</p><p><strong>同时可以比较容易看出，可以使用蒙特卡罗方法进行估计的前提是两个状态之间转移的奖励必须是已知的，否则无法进行计算！</strong></p>          </div><h2 id="时序差分方法">2.时序差分方法</h2><h3 id="总体方法概述">2.1 总体方法概述</h3><p><strong>时序差分方法（ Temporal Difference methods，TD）</strong> 是另外一种模型无关的，直接使用经验学习的方法。时序差分方法的核心公式如下：<img src="/RL2pictures/image-13.png" alt="alt text" /> 在时序差分中，我们通过当前行为带来的即时奖励以及下一个回合的V值与<span class="math inline"><em>γ</em></span>（也就是，对于未来的估计值）来更新当前状态的价值V</p><div class="note note-primary">            <p>这里使用学习率 <strong><span class="math inline"><em>α</em></span></strong> 来控制更新大小的大小，是为了避免产生过度更新的问题，比如更新值过大导致来回震荡，而加上一个系数可以保证平滑更新</p>          </div><h3 id="时序差分-vs-蒙特卡洛">2.2 时序差分 vs 蒙特卡洛</h3><p>（1）从上面的定义就可以看出，时序差分方法在每一次执行动作之后都会利用下一个状态的<span class="math inline"><em>V</em></span>值更新（每一步后可以进行在线学习），而蒙特卡洛则需要跑完一个回合之后才会利用累计奖励对于<span class="math inline"><em>V</em>(<em>s</em>)</span>值进行更新 （2）很容易知道蒙特卡洛方法一定是<strong>无偏估计</strong>，而时序差分方法由于使用的就是下一个状态的猜测值<span class="math inline"><em>V</em>(<em>s</em><sub><em>t</em> + 1</sub>)</span>而不是该策略下的真实值<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em><sub><em>t</em> + 1</sub>)</span>。因此，TD是有偏估计。 （3）时序差分方法对于初始值更加敏感，而蒙特卡洛方法对于初始值不敏感 （4）蒙特卡洛方法得到的结果的方差更大，而时序差分方法得到的结果方差更小 总结下来如图所示：<img src="/RL2pictures/image-14.png" alt="alt text" /></p><h2 id="资格迹方法">3. 资格迹方法</h2><h3 id="总体方法概述-1">3.1 总体方法概述</h3><p>如果说把使用窗口的蒙特卡洛方法中的窗口大小倒数<span class="math inline"><em>α</em></span>看作类似于“学习率”的参数，那么前面三个方法的关系可以这样概括：时序差分是只参考当前下一步的估计方法，资格迹方法是参考下面若干步的多步时序差分方法，而蒙特卡洛方法则是参考未来无限步（直到回合结束）的方法。</p><p>因此，<strong>资格迹方法（Eligibility Traces methods）</strong> 可以看作是前面两种方法的一个平衡，其平衡了方差和偏差的关系，是一种比较折中的方案。依葫芦画瓢得到如下的公式：<img src="/RL2pictures/image-15.png" alt="alt text" /></p><h3 id="具体实现">3.2 具体实现</h3><h4 id="td-lambda-方法">3.2.1 <span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span> 方法</h4><p>所谓的 <strong><span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span>方法</strong>，是指使用一个超参数<span class="math inline"><em>λ</em></span> 进行控制后续使用的各个阶段G的权重，具体如下：<img src="/RL2pictures/image-16.png" alt="alt text" /> 解释一下：<span class="math inline"><em>G</em><sub><em>t</em></sub><sup><em>n</em></sup></span> 指的是（上面所说的）考虑了之后n步具体奖励的资格迹方法；第二个公式实际上是一个加权平均（注意到：<strong><span class="math inline"><em>λ</em></span>+<span class="math inline"><em>λ</em><sup>2</sup></span> +…+<span class="math inline"><em>λ</em><sup><em>n</em></sup></span> =<span class="math inline">$\frac{1}{1 - \lambda}$</span></strong> ）。可以看到，距离现在时间越远的远期奖励权重会越来越小，而参数<span class="math inline"><em>λ</em></span>控制着这个权重衰减的速率。</p><p>这种方法较好地平衡了MC和TD，其不仅会有更快的收敛速度（相比于MC），而且平衡了偏差和方差之间的关系；同时，保留了大量的历史信息的加权结果，使得对于价值的估计更加全面。</p><div class="note note-danger">            <p>下面是两张直观反映<span class="math inline"><em>T</em><em>D</em> − <em>λ</em></span>方法的图片：<img src="/RL2pictures/image-17.png" alt="alt text" /><img src="/RL2pictures/image-18.png" alt="alt text" /> 但是暂时不是很理解具体的意思，不知道这个”后向视角“具体是怎么结合的</p>          </div><h2 id="表格型时序差分方法sarsa-q-learning">4. 表格型时序差分方法：SARSA &amp; Q-learning</h2><h3 id="核心思想">4.1 核心思想</h3><p>强化学习的核心部分便是<strong>策略评估和策略优化</strong> （recall：之前dp的策略迭代和策略评估），具体都在下图中有所指出：<img src="/RL2pictures/image-19.png" alt="alt text" /> 也就是说，策略评估就是估算出<span class="math inline"><em>V</em></span>,(也就是之前几个部分在做的)，之后再根据算出来的<span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>，来选择最好的策略（动作）</p><h3 id="sarsa">4.2 SARSA</h3><h4 id="总体方法概述-2">4.2.1 总体方法概述</h4><p>所谓的SARSA，指的是<strong>S</strong>tate-<strong>A</strong>ction-<strong>R</strong>eward-<strong>S</strong>tate-<strong>A</strong>ction。先来看一段伪代码，直观反映了这个方法的运行步骤：<img src="/RL2pictures/image-21.png" alt="alt text" /> 作为背景，首先介绍 <strong><span class="math inline"><em>ϵ</em> − <em>g</em><em>r</em><em>e</em><em>e</em><em>d</em><em>y</em></span></strong> 方法：这是一种在强化学习中常用的<strong>平衡探索与利用</strong>的手段。在每一个时间步中，agent都会以<span class="math inline"><em>ϵ</em></span>的概率随机选择一个动作（实现对未知的探索），以<span class="math inline">1 − <em>ϵ</em></span>的概率选择当前的最优策略（贪心，实现对现在的利用）<br />接着，基于上面策略评估与策略优化的思想，SARSA分为如下几个步骤实现： <strong>step1：</strong> 初始化参数值，包括初始化各个<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>的值，以及初始化每个步骤用于选择的<span class="math inline"><em>ϵ</em></span>初始值</p><p><strong>step2：</strong> 从初始环境<span class="math inline"><em>s</em><sub>0</sub></span>开始，在每一个状态<span class="math inline"><em>s</em></span>处，按照<span class="math inline"><em>ϵ</em></span>-greedy策略进行选择动作<span class="math inline"><em>a</em><sub><em>t</em></sub></span>（即，可能选择当前的最佳，也有可能随即探索），并且根据这个行为观察奖励<span class="math inline"><em>R</em></span>；随后进入下一个状态<span class="math inline"><em>s</em>′</span>,并且再利用一次<span class="math inline"><em>ϵ</em></span>-greedy策略进行选择动作<span class="math inline"><em>a</em>′</span>，并且读取对应的$Q(s’,a’)的值</p><p><strong>step3：</strong> 利用上途中的公式，更新<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>的值；并且一直重复这个过程，直到Q值收敛或者状态结束</p><h4 id="原理是什么">4.2.2 原理是什么？</h4><p>对于SARSA的原理，可以把它看成是针对当前策略的一个 <strong><span class="math inline"><em>T</em><em>D</em></span>的更新</strong>，因为在更新公式中，可以把 <strong><span class="math inline"><em>R</em> + <em>γ</em><em>Q</em>(<em>s</em>′, <em>a</em>′)</span></strong> 这一项看作是对于当前策略奖励（更）准确的值（因为多向前跑了一项），而 <strong><span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span></strong> 则可以看作是当前状态下对于这样一个行为结果的一个预测，因此对于两者给上一个学习率系数就可以实现学习的过程。</p><div class="note note-primary">            <p>SARSA的一个性质是，随着训练的进行，模型会越来越快抵达最后的“目标”，即最后比较接近的预测值。</p>          </div><h3 id="q-learning">4.3 Q-learning</h3><h4 id="基本方法概述">4.3.1 基本方法概述</h4><p>SARSA是一种典型的 <strong>on-policy</strong> 方法，即，在进行学习过程中使用的策略就是智能体使用的策略；而与之不同的是，Q-learning是一种 <strong>off-policy</strong> 方法，这个的意思是，学习的策略与当前使用的策略不同，而是先“贷款”了未来会学习到的最优策略进行优化。 Q-learning的核心是如下的这个公式：<img src="/RL2pictures/image-23.png" alt="alt text" /> 可以注意到，和之前的SARSA不同，这里更新的下一步得到奖励使用的是“期望来到下一个状态之后所取得的最优步骤（即为最大的奖励），其他的结构是一样的。（recall之前的SARSA，区别就是SARSA只会以<span class="math inline"><em>ϵ</em></span>的概率随机选择状态<span class="math inline"><em>s</em>′</span>最优的动作）</p><h4 id="具体实现方法-1">4.3.2 具体实现方法</h4><p>可以把Q-learning看作简单版本的SARSA，其每一步都会默认选择最优的策略（而非按照一定的概率）来更新数值，根据数学的定理其一定会收敛到最后的最优策略Q*</p><div class="note note-primary">            <p>这是一个直观展现Q-learning和SARSA两个不同特点的例子：<img src="/RL2pictures/image-24.png" alt="alt text" /> 由于Q-learning倾向于选择”最佳的“，所以其会贴着悬崖走；而SARSA会按照一个更加探索过的路线行走。</p>          </div><div class="note note-success">            <p>总结一下这个部分的内容：和标题一样，这个部分主要介绍了强化学习中的值估计部分的内容，从一开始的MC和TD以及资格迹是对于V值估计的三个方法，到后面的两个表格式TD对于每一个状态-动作组的Q值的估计，这个部分在RL1的基础上进一步展示了当环境对我们更为陌生的时候应该如何对于各个状态和行为的价值进行估计。</p>          </div>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
      <tag>蒙特卡洛</tag>
      
      <tag>时序差分</tag>
      
      <tag>资格迹</tag>
      
      <tag>Q-learning</tag>
      
      <tag>SARSA</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL0</title>
    <link href="/2025/08/06/RL0/"/>
    <url>/2025/08/06/RL0/</url>
    
    <content type="html"><![CDATA[<h1 id="rl0-基础数学知识">RL0： 基础数学知识</h1><h2 id="无偏估计">1.无偏估计</h2><p>在统计学中，无偏估计指的是一个估计量的期望值等于其真实值，即 <strong><span class="math inline"><em>E</em>[<em>θ̂</em>] = <em>θ</em></span></strong> 在强化学习的状态估计里面，无偏估计即为我们估计到的<span class="math inline"><em>V</em>(<em>s</em>)</span>与其真实值相同的情况</p><h2 id="梯度">2.梯度</h2><p>recall：对于一个多元函数，其梯度是一个<strong>向量</strong>，为该函数对于每一个变量求偏导数的结果，即为 <span class="math inline">$\nabla f(x_1, x_2, \dots, x_n) = \left( \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, \dots, \frac{\partial f}{\partial x_n} \right)$</span></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
  </entry>
  
  
  
  <entry>
    <title>RL1：强化学习基本概念，马尔科夫决策过程，DP</title>
    <link href="/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/"/>
    <url>/RL1%EF%BC%9A%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5%EF%BC%8C%E9%A9%AC%E5%B0%94%E7%A7%91%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%8CDP/</url>
    
    <content type="html"><![CDATA[<h2 id="rl1强化学习基本概念马尔科夫决策过程dp"># RL1：强化学习基本概念，马尔科夫决策过程，DP</h2><h2 id="基本定义">1. 基本定义</h2><ul><li>强化学习是机器学习的一个分支，专注于让智能体（Agent）通过与环境的交互学习最优策略，以最大化累积奖励。其核心思想是试错学习（Trial-and-Error），类似于人类或动物通过经验改进行为的过程。</li></ul><p>强化学习的主体为<strong>智能体(agent)</strong>，即为对环境做出感知、决策、行动的对象 <em>（e.g. 玩游戏时的角色&amp;自动驾驶的汽车）</em>；<strong>环境(environment)</strong> 是与智能体交互的一切外界规则与机理 <em>（即为智能体所处的“世界”）</em>，</p><div class="note note-warning">            <h4 id="q-价值学习和策略学习分别是什么">Q: 价值学习和策略学习分别是什么？</h4><p>A:（以后慢慢补充）</p>          </div><hr /><h2 id="马尔科夫决策过程markov-decision-processmdp">2. 马尔科夫决策过程(Markov decision process，MDP)</h2><h3 id="mdp基本五要素">2.1 MDP基本五要素</h3><p>MDP是可以通过强化学习解决问题的基础建模，其由以下的五个元素组成𝑀 =&lt; 𝑆,𝐴,𝑃,𝑅,𝛾 &gt;</p><ul><li>状态（State）集合 <strong>S</strong>: 状态指的是当前时刻，对于整个环境一帧画面的概括 <em>(e.g.下棋一个时刻棋盘所有棋子的位置&amp;玩游戏时某时刻的一帧画面)</em></li><li>动作（Action）集合 <strong>A</strong>: （我认为）<strong>动作必须依赖于特定的状态</strong>，动作集合是agent在当前状态下可能做出的所有决策</li><li>状态转移函数<strong>P</strong>: 状态转移取决于当前的状态和行为，并且具有随机性，概率用<strong>状态转移函数</strong> 𝑝(𝑠’|𝑠,𝑎)进行衡量，指的是agent在当前状态s采取行动a转移到状态s’的概率。</li></ul><div class="note note-warning">            <p>要注意状态转移具有<strong>随机性</strong>，由于环境的不确定性和干扰，当前agent即使在给定时刻采取确切的行为，也无法确定下面的状态。下面是一个通俗的例子<img src="/RL1pictures/image.png" /></p>          </div><ul><li>奖励函数<strong>R</strong>: 人为设置的奖励对于训练模型很重要，通常来说需要设置一系列的<strong>r(s,a,s’)</strong> <em>(从状态s采取行为a转移到状态s’的奖励)</em> 给智能体，以达到最终的训练目的；有时奖励会退化为r(s,a) <div class="note note-primary">            <h4 id="r_t与rsa的区别"><span class="math inline"><em>R</em><sub><em>t</em></sub></span>与<span class="math inline"><em>r</em>(<em>s</em>, <em>a</em>)</span>的区别：</h4><p>依旧让gpt来说： &gt; <span class="math inline"><em>r</em>(<em>s</em>, <em>a</em>)</span>: &gt; 表示 <strong>某个状态下，执行某个动作后得到的即时奖励</strong>。 它更像是一个 <strong>奖励函数</strong>，给定状态 <span class="math inline"><em>s</em></span> 和动作<span class="math inline"><em>a</em></span>，返回一个奖励值。 它是环境对动作的即时反馈，通常与时间步无关。</p><blockquote><p><span class="math inline"><em>r</em><sub><em>t</em></sub></span> : 表示在 <strong>特定时间步 <span class="math inline"><em>t</em></span></strong> 下，智能体执行某个动作后得到的即时奖励。 它是 <strong>即时奖励的时刻标记</strong>，指在时刻<span class="math inline"><em>t</em></span> 上的反馈。 这个奖励可能与某个特定的状态或动作有关，通常用于标识当前时间步的奖励。 所以说，主要就是衡量的参数不同（状态、行为 or 时间）</p></blockquote>          </div></li></ul><div class="note note-danger">            <p>缺一个MDP矩阵形式的解，但感觉没啥用，以后再补充</p>          </div><ul><li>折扣因子<strong>𝛾</strong>: 一个介于0和1之间的参数，由于（人之常情）需要多关注眼前立刻获得的回报，因此对于未来若干步之后才能取得的好处就要设置折扣因子来减小权重</li></ul><h3 id="回报价值函数策略">2.2 回报，价值函数，策略</h3><p>首先说明<strong>回合(episode)</strong> 的概念：一个“回合”指的是智能体从初始状态与环境交互直到terminal state的过程，就是 <strong>一局游戏，一次任务从开始到执行结束</strong>.</p><ol type="1"><li><p><strong>回报（reward）</strong>: 回报 <strong><span class="math inline"><em>G</em><sub><em>t</em></sub></span></strong> 实际上是一个统计量，指的是一个回合后（一局游戏结束后），由于所有经过的状态、行为和每一步的奖励都确定了，<span class="math inline"><em>G</em><sub><em>t</em></sub></span> 统计了整个回合（或者有限视野内）从t时刻开始所有奖励的累计值，即为<img src="/RL1pictures/image-2.png" /> 因此，回报是一个确定的值</p></li><li><p><strong>价值函数</strong>: 一个状态的期望回报即为这个状态的价值；价值函数 <strong><span class="math inline"><em>V</em>(<em>s</em>)</span></strong> 输入一个状态，输出这个状态的期望回报；价值函数可以写为 <span class="math inline"><em>V</em>(<em>s</em>) = <em>E</em>[<em>G</em><sub><em>t</em></sub>|<em>S</em><sub><em>t</em></sub> = <em>s</em>]</span>。进一步地，将其展开得到：<img src="/RL1pictures/image-3.png" alt="alt text" /></p></li></ol><div class="note note-primary">            <p>对于价值函数的定义式，在引入策略 <span class="math inline"><em>π</em></span>之前，感觉有些抽象，因为状态转移之间没有规律可寻，gpt这么说： &gt; 在 强化学习 或 马尔可夫决策过程（MDP）的背景下，价值函数 𝑉(𝑠)主要用来评估每个状态的“好坏”。如果没有显式的策略𝜋，通常假设状态值函数 V(s) 是在某个<strong>默认策略</strong>下定义的，或者更常见的情况是我们关心的是<strong>最优策略</strong>。</p><p>因此，我觉得V是用来衡量状态“好坏”的抽象概念，具体要结合策略才有实际作用</p>          </div><ol start="3" type="1"><li><strong>策略(policy), <span class="math inline"><em>π</em></span></strong> 策略是智能体通过判断环境和状态，来决定下一步动作的函数。通常来说，策略分为<strong>确定性策略</strong>（一个状态对应唯一的动作：比如说，走迷宫设定策略为一直往前冲，除非看到墙就直接右拐）和<strong>随机性策略</strong>。在机器学习中，我们主要讨论随机性策略。 在随机性策略中,核心内容为概率密度函数 <strong><span class="math inline"><em>π</em>(<em>a</em>|<em>s</em>)</span></strong> ，其得到的结果为一个概率分布，为在当前策略&amp;状态s下，采取动作a的概率，即 <span class="math inline"><em>P</em>(<em>a</em><sub><em>t</em></sub> = <em>t</em>|<em>s</em><sub><em>t</em></sub> = <em>s</em>)</span>。</li></ol><div class="note note-primary">            <p><strong>采样(sample)</strong> 和 <strong>轨迹(trajectory)</strong> 是另外两个比较重要的概念。gpt这样告诉我： &gt; 在 强化学习 中，采样（sample）指的是从环境中收集单一数据点或一小段数据。通常来说，采样是从当前状态中通过采取一个动作获得的反馈信息。这些数据（状态、动作、奖励、下一个状态）通常是通过与环境交互而获得的一个时间步的数据点。每次从环境中收集到的一组（状态、动作、奖励、下一个状态）就是一个 采样。 轨迹（trajectory）通常指的是智能体从某一初始状态出发，通过多次采样所得到的一系列数据点。它是智能体在一段时间内与环境交互的整个过程。</p>          </div><h3 id="v与q-bellman">2.3 V与Q, Bellman</h3><ol start="4" type="1"><li><strong>MDP的价值和回报</strong>：结合上面给出的策略概念，在使用策略<span class="math inline"><em>π</em></span>的前提下，我们很容易得到在相应策略下一个<strong>状态价值函数</strong> <span class="math inline"><em>V</em><sup><em>π</em></sup>(<em>s</em>)</span>的表达式 <img src="/RL1pictures/image-4.png" alt="alt text" /> 进一步地，定义<strong>价值-行为函数</strong> <span class="math inline"><em>Q</em><sup><em>π</em></sup>(<em>s</em>, <em>a</em>)</span>，指的是agent在采用策略<span class="math inline"><em>π</em></span>的大方向下，在当前状态s已经采取特定行为a之后，期望得到的总回报。</li></ol><div class="note note-primary">            <h3 id="v与q的关系">V与Q的关系</h3><p>根据上述的定义，可以比较容易得到V与Q之间的计算关系<img src="/RL1pictures/image-5.png" alt="alt text" /> 通俗地说，在给定的策略下，<strong>V为给定状态s的期望价值，Q为给定状态s和行为a的期望价值</strong>；因此V即为<span class="math inline"><em>π</em></span>的概率分布下各个行为期望价值的加权和。 从另一个角度看：<img src="/RL1pictures/image-6.png" alt="alt text" /> 就是拆一步当前行为、状态的即时奖励，再加上当前(s,a)转移到各个新状态的V回报的加权和。（当然还有<span class="math inline"><em>γ</em></span>） <strong>简单来说，V和Q都是回报，只是涉及的变量不同</strong></p>          </div><p>5.<strong>贝尔曼期望方程，贝尔曼最优方程：策略评估与策略优化</strong> （1）贝尔曼期望方程：<img src="/RL1pictures/image-7.png" alt="alt text" /> 数学上来看，贝尔曼期望方程就是根据上面V与Q的关系转化为只有V和Q的形式。我理解其本质是一个“向后迭代一步”的方程，其<strong>将最后的回报转化为仅仅与下一步所有回报值的加权平均</strong>，同时也方便迭代递归求值 （2）贝尔曼最优方程：<img src="/RL1pictures/image-8.png" alt="alt text" /> 如果说贝尔曼期望方程是针对当前的策略计算值，那么贝尔曼最优方程是仅仅关注最优值，即在迭代过程中，不关心策略，而是每一步贪心，选择能让下一步回报最大的动作。</p><hr /><h2 id="动态规划dynamic-programming">3. 动态规划(dynamic programming)</h2><h3 id="introduction">3.1 introduction</h3><p>在RL领域，动态规划主要用来解决一类<strong>已知环境模型</strong>的问题，结合上述的贝尔曼方程与DP的将全局寻求最优转化为每一步寻求为最优的思想解决问题。（后面会很容易看出为什么要这样） 首先定义一系列符号： 状态集合表示为 {0（终止状态）,1,2…n}，每个状态对应的可选动作集为 <strong><span class="math inline"><em>A</em>(<em>i</em>)</span></strong> <strong><span class="math inline"><em>p</em><sub><em>i</em>, <em>j</em></sub>(<em>a</em>)</span></strong> 表示在状态i,采取行动a转移到状态j的概率，<strong><span class="math inline"><em>r</em><sub><em>i</em>, <em>j</em></sub>(<em>a</em>)</span></strong> 表示在状态i,采取行动a转移到状态j的奖励，（前面还有k步后的折扣因子<span class="math inline"><em>γ</em><sup><em>k</em></sup></span>）</p><p>动态规划期望得到一个最优的策略，即在每一个状态<span class="math inline"><em>i</em></span>下，都能给出一个当下最优的动作<span class="math inline"><em>μ</em>(<em>i</em>) ∈ <em>A</em>(<em>i</em>)</span></p><h3 id="基于dp的强化学习算法bellman方程的应用">3.2 基于dp的强化学习算法————Bellman方程的应用</h3><h4 id="策略迭代policy-iteration">3.2.1 策略迭代(policy iteration)</h4><p>对于策略迭代，一般分为如下的两个步骤交替进行<img src="/RL1pictures/image-9.png" alt="alt text" /></p><p>第一步的 <strong>策略评估</strong> 中，采取当前的策略，一般会为所有的状态定义一个（随机的）初始V值，之后不断使用Bellman期望方程进行迭代，直到收敛为止，以得到各个状态真实的奖励V值。 <div class="note note-danger">            <p>初始的策略应该可以完全随机设置；初始的V值感觉也可以随机设置，是否有影响存疑（我觉得无所谓）；同时，强大的<a href="https://baike.baidu.com/item/%E5%B7%B4%E6%8B%BF%E8%B5%AB%E4%B8%8D%E5%8A%A8%E7%82%B9%E5%AE%9A%E7%90%86/9492042"><strong><u>Banach不动点定理</u></strong></a>保证一定可以得到最终的收敛解决；如下的简单推导确保优化结果一定越来越好<img src="/RL1pictures/image-10.png" alt="alt text" /></p>          </div> 得到所有的V值之后，可以相应对于每一个状态s得到所有动作a的Q值（也是根据Bellman方程的公式），之后，进入<strong>策略优化</strong> 阶段，我们选取 <strong><span class="math inline">$\pi(s)=\arg\max\limits_a Q(s, a)$</span></strong> ,即直接用得到Q最大的动作a来作为新的策略 之后不断重复这两步，计算新的V Q，更新策略…… 直到策略收敛不再改变为止。</p><p><strong>更简单地说，贝尔曼方程就是用来求出V值，然后根据V求出所有的Q值，最后根据Q值来更新策略</strong></p><div class="note note-danger">            <p>初学时陷入了一个误区：既然算出V后可以得到获得最好Q的行为a，那为什么不直接选择这个行为作为最佳策略？ 其实也很简单解决，因为这只是目前<span class="math inline"><em>π</em></span>看到的所谓好行为，但对于别的策略并非最优</p>          </div><h4 id="价值迭代value-iteration">3.2.2 价值迭代（value iteration）</h4><p>核心表达式为 <strong><span class="math inline">$V(s) \leftarrow \max\limits_a \left[ r(s, a) + \gamma \sum_{s'} P(s' | s, a) V(s') \right]$</span></strong> ，通俗说，策略迭代需要计算所有行为得到的奖励再加权相加作为当前状态的V，而价值迭代直接把奖励低的pass掉，直接把奖励最高的按100%权重给当前的状态。 就这样，得到新的一轮V，重复这样的方法直到得到的V收敛即为最终确切的状态，然后再根据 <strong><span class="math inline">$\pi^*(s) = \arg\max\limits_a \left[ r(s, a) + \gamma \sum_{s'} P(s' | s, a) V^*(s') \right]$</span></strong> 反推出策略即可 <div class="note note-success">            <p>总结一下Bellman方程与DP: 结合具体的例子看Bellman方程比较好理解，期望方程就是把所有对应的<span class="math inline"><em>Q</em>(<em>s</em>, <em>a</em>)</span>加权得到新的V值，而最优方程就是把结果最好的<span class="math inline"><em>Q</em><sup>*</sup>(<em>s</em>, <em>a</em>)</span>直接作为新的V值；第一种方法对应不断迭代得到收敛的V值再进行优化的策略迭代算法，而由于这个阶段的巨大计算量，价值迭代则直接用最优的替代来节约计算时间。 然而，由于DP算法对于解决问题的要求很苛刻（要求所有的状态转移概率和获得奖励值全部已知），而大多数问题都显然比这个复杂，因此局限性还是很大的。</p>          </div></p>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
      <tag>马尔科夫决策过程</tag>
      
      <tag>动态规划</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RL home</title>
    <link href="/2025/08/05/RL-notes/"/>
    <url>/2025/08/05/RL-notes/</url>
    
    <content type="html"><![CDATA[<h1 id="about-rl">About RL</h1><p>暂定用来记录一些之前的笔记和后面自学的强化学习相关的内容。</p><h2 id="references">References</h2><p>1.王树森 深度强化学习<br />2. 多智能体强化学习短学期ppt</p><div class="note note-success">            <h3 id="目录">目录</h3><p><a href="/RL1：强化学习基本概念，马尔科夫决策过程，DP/"><u>RL1：强化学习基本概念，马尔科夫决策过程，DP</u></a></p><p><a href="/RL2：Value-estimation/"><u>RL2：Value estimation</u></a> <a href="/RL3：深度学习&amp;DQN/"><u>RL3：深度学习&amp;DQN</u></a></p>          </div>]]></content>
    
    
    <categories>
      
      <category>强化学习</category>
      
    </categories>
    
    
    <tags>
      
      <tag>强化学习</tag>
      
      <tag>笔记</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>hexo 常用命令&amp;指南</title>
    <link href="/2025/08/04/hello-world/"/>
    <url>/2025/08/04/hello-world/</url>
    
    <content type="html"><![CDATA[<h2 id="quick-start">Quick Start</h2><h3 id="create-a-new-post">Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="run-server">Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="generate-static-files">Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="deploy-to-remote-sites">Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p><p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
